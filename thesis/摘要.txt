       知识蒸馏是将一个神经网络的隐含知识迁移到另一个神经网络的过程，其中提供知识的神经网络称为教师模型，学习知识的神经网络称为学生模型。相比于传统的知识蒸馏模型，自知识蒸馏模型不需要外部的教师模型，而是把神经网络最深层视为教师模型，把神经网络的浅层视为学生模型来达到知识蒸馏的效果。
       现有的自知识蒸馏模型（如BYOT模型）将作为学生模型的各个浅层块一视同仁，忽略了各个浅层块对最深层块的不同影响。通过分析BYOT模型的原理和不足，首先提出了一种基于逐块衰减的改进BYOT模型（Per-block Decay based BYOT, PD-BYOT），通过在BYOT模型的各浅层块添加构成等比数列的衰减系数，以达到区分各浅层块对最深层块影响的目的。
       然后，在BYOT模型和所提出的改进PD-BYOT模型的基础上，提出了一种新的自知识蒸馏模型，即基于自注意力机制的自知识蒸馏模型（Self-Knowledge Distillation with Self-Attention Mechanism, SKDSAM）。SKDSAM模型通过在BYOT模型的每一个浅层块和最深层块之间添加自注意力连接，计算出相应的注意力权重，从而准确地量化各浅层块对最深层块的不同影响。SKDSAM还修正了BYOT模型的两种损失函数，以便更有效地提取知识蒸馏模型中的暗知识（Dark Knowledge）。随后，从理论上证明了SKDSAM模型中的自注意力机制等价于集成学习中的装袋法，证实了SKDSAM模型具有更强的稳定性和抗过拟合能力。最后，将SKDSAM模型与三种数据增强技术（Cutout、SLA及Mixup）相结合，以进一步提升模型的性能。
       实验结果表明，PD-BYOT模型相比于BYOT模型在性能上有一定的提升。进一步地，SKDSAM模型在多个图像数据集上取得了相比于现有的其他的自知识蒸馏模型更高的分类准确率。通过消融实验，说明了自注意力机制和自注意力机制中的知识蒸馏模块对提升SKDSAM模型性能的作用，以及结合数据增强能够进一步提升SKDSAM模型的性能。 

   Knowledge distillation is the process of transferring implicit knowledge from one neural network to another. The network providing knowledge is called the teacher, and the network receiving knowledge is called the student. Unlike traditional knowledge distillation, the self-knowledge distillation model distills knowledge from the deepest layer (acting as the teacher) to shallow layers (acting as the student) without an outside teacher model. 
   Current self-knowledge distillation techniques (i.e., BYOT) treat all shallow layers (acting as students) equally, neglecting their different impacts on the deepest layer. Through analyzing the mechanism and shortcomings of BYOT, we first propose the Per-block Decay based BYOT model (PD-BYOT), which utilizes the geometric progression of attenuation coefficient to differentiate each shallow layer’s impact on the deepest layer. 
   On the basis of the model BYOT and the proposed improved model PD-BYOT, we further put forward a novel framework, which we refer to as Self-Knowledge Distillation with Self-Attention Mechanism (SKDSAM). It adds attention links between each shallow layer and the deepest layer of BYOT and computes the attention weight of each shallow layer so as to quantify the contribution of each shallow layer to the deepest layer. The SKDSAM model also modifies two BYOT’s loss functions to mine the network’s dark knowledge more efficiently. We then provide theoretical proof that the self-attention mechanism in SKDSAM is essentially an ensemble modeling strategy (namely Bagging), which means SKDSAM has the advantage of robustness and preventing overfitting. Moreover, we combine the SKDSAM model with three data augmentation techniques (Cutout, SLA and Mixup) to further improve model performance.
     The experimental results show that the PD-BYOT model slightly improves the classification accuracy compared with the BYOT model. Furthermore, the SKDSAM model outperforms all the self-distillation models on various image datasets. The ablation study proves the importance of the self-attention mechanism and temperature scaling in the self-attention mechanism. The ablation study also shows that combining the SKDSAM model and data augmentation techniques can further improve performance. 
