opt = Namespace(T=2, alpha=0.1, batch_size=128, beta=1e-05, cuda=1, dataset='cifar100', decay_fea=1, decay_kd=1, decay_loss=1, distill='kd', epochs=240, lr=0.1, model='multi_resnet50_kd', model_name='multi_resnet50_kd_cifar100_selfkd_origin', model_path='../save/models', momentum=0.9, num_workers=8, print_freq=100, save_folder='../save/models/multi_resnet50_kd_cifar100_selfkd_origin', save_freq=40, seed=2, tb_folder='../save/tensorboard/multi_resnet50_kd_cifar100_selfkd_origin', tb_freq=500, tb_path='../save/tensorboard', weight_decay=0.0001)
----------- Network Initialization --------------
model = Multi_ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BottleneckBlock(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BottleneckBlock(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BottleneckBlock(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BottleneckBlock(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BottleneckBlock(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BottleneckBlock(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BottleneckBlock(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BottleneckBlock(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BottleneckBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BottleneckBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BottleneckBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BottleneckBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BottleneckBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BottleneckBlock(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BottleneckBlock(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BottleneckBlock(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (downsample1_1): Sequential(
    (0): Conv2d(256, 2048, kernel_size=(1, 1), stride=(8, 8), bias=False)
    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (bottleneck1_1): Sequential(
    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(512, 512, kernel_size=(8, 8), stride=(8, 8))
    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))
    (7): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
  )
  (avgpool1): AdaptiveAvgPool2d(output_size=(1, 1))
  (middle_fc1): Linear(in_features=2048, out_features=100, bias=True)
  (downsample2_1): Sequential(
    (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(4, 4), bias=False)
    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (bottleneck2_1): Sequential(
    (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(512, 512, kernel_size=(4, 4), stride=(4, 4))
    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))
    (7): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
  )
  (avgpool2): AdaptiveAvgPool2d(output_size=(1, 1))
  (middle_fc2): Linear(in_features=2048, out_features=100, bias=True)
  (downsample3_1): Sequential(
    (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (bottleneck3_1): Sequential(
    (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(512, 512, kernel_size=(2, 2), stride=(2, 2))
    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))
    (7): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
  )
  (avgpool3): AdaptiveAvgPool2d(output_size=(1, 1))
  (middle_fc3): Linear(in_features=2048, out_features=100, bias=True)
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=100, bias=True)
)
param size = 54.113232MB
-----------------------------------------------
 Save initial parameters
Epoch: 1  lr: 0.100
Epoch: [1][000/391]	Time 2.5334 Data 0.8151 Loss 4.8249 (4.8249)	Acc@1 0.000 (0.000)	Acc@5 3.125 (3.125)
opt = Namespace(T=2, alpha=0.1, batch_size=128, beta=1e-05, cuda=1, dataset='cifar100', decay_fea=1, decay_kd=1, decay_loss=1, distill='kd', epochs=240, lr=0.1, model='multi_resnet50_kd', model_name='multi_resnet50_kd_cifar100_selfkd_origin', model_path='../save/models', momentum=0.9, num_workers=8, print_freq=100, save_folder='../save/models/multi_resnet50_kd_cifar100_selfkd_origin', save_freq=40, seed=2, tb_folder='../save/tensorboard/multi_resnet50_kd_cifar100_selfkd_origin', tb_freq=500, tb_path='../save/tensorboard', weight_decay=0.0001)
----------- Network Initialization --------------
model = Multi_ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BottleneckBlock(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BottleneckBlock(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BottleneckBlock(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BottleneckBlock(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BottleneckBlock(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BottleneckBlock(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BottleneckBlock(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BottleneckBlock(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BottleneckBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BottleneckBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BottleneckBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BottleneckBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BottleneckBlock(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BottleneckBlock(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BottleneckBlock(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BottleneckBlock(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (downsample1_1): Sequential(
    (0): Conv2d(256, 2048, kernel_size=(1, 1), stride=(8, 8), bias=False)
    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (bottleneck1_1): Sequential(
    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(512, 512, kernel_size=(8, 8), stride=(8, 8))
    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))
    (7): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
  )
  (avgpool1): AdaptiveAvgPool2d(output_size=(1, 1))
  (middle_fc1): Linear(in_features=2048, out_features=100, bias=True)
  (downsample2_1): Sequential(
    (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(4, 4), bias=False)
    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (bottleneck2_1): Sequential(
    (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(512, 512, kernel_size=(4, 4), stride=(4, 4))
    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))
    (7): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
  )
  (avgpool2): AdaptiveAvgPool2d(output_size=(1, 1))
  (middle_fc2): Linear(in_features=2048, out_features=100, bias=True)
  (downsample3_1): Sequential(
    (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (bottleneck3_1): Sequential(
    (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(512, 512, kernel_size=(2, 2), stride=(2, 2))
    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU()
    (6): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))
    (7): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU()
  )
  (avgpool3): AdaptiveAvgPool2d(output_size=(1, 1))
  (middle_fc3): Linear(in_features=2048, out_features=100, bias=True)
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=100, bias=True)
)
param size = 54.113232MB
-----------------------------------------------
 Save initial parameters
Epoch: 1  lr: 0.100
Epoch: [1][000/391]	Time 2.4241 Data 0.8349 Loss 4.8249 (4.8249)	Acc@1 0.000 (0.000)	Acc@5 3.125 (3.125)
Epoch: [1][100/391]	Time 0.1881 Data 0.0028 Loss 4.6184 (5.4845)	Acc@1 3.906 (1.083)	Acc@5 11.719 (5.283)
Epoch: [1][200/391]	Time 0.1889 Data 0.0027 Loss 4.6017 (5.0788)	Acc@1 2.344 (1.046)	Acc@5 7.031 (5.193)
Epoch: [1][300/391]	Time 0.1890 Data 0.0025 Loss 4.5859 (4.9271)	Acc@1 0.781 (1.095)	Acc@5 4.688 (5.381)
Testing the models......
Loss: 4.5603, Prec@1: 2.11, Prec@5: 9.40
Epoch time: 84s
Saving models
Epoch: 2  lr: 0.100
Epoch: [2][000/391]	Time 1.1746 Data 0.9453 Loss 4.5651 (4.5651)	Acc@1 1.562 (1.562)	Acc@5 9.375 (9.375)
Epoch: [2][100/391]	Time 0.1895 Data 0.0029 Loss 4.3858 (4.5467)	Acc@1 4.688 (2.359)	Acc@5 18.750 (10.466)
Epoch: [2][200/391]	Time 0.1933 Data 0.0027 Loss 4.3508 (4.5033)	Acc@1 0.781 (2.713)	Acc@5 13.281 (12.131)
Epoch: [2][300/391]	Time 0.1892 Data 0.0022 Loss 4.3105 (4.4716)	Acc@1 5.469 (3.000)	Acc@5 17.188 (13.416)
Testing the models......
Loss: 4.5807, Prec@1: 4.77, Prec@5: 19.59
Epoch time: 82s
Saving models
Epoch: 3  lr: 0.100
Epoch: [3][000/391]	Time 1.0500 Data 0.8675 Loss 4.2637 (4.2637)	Acc@1 3.125 (3.125)	Acc@5 21.875 (21.875)
Epoch: [3][100/391]	Time 0.1891 Data 0.0022 Loss 4.6361 (4.3192)	Acc@1 6.250 (5.043)	Acc@5 16.406 (20.815)
Epoch: [3][200/391]	Time 0.1895 Data 0.0023 Loss 4.0172 (4.3111)	Acc@1 6.250 (5.379)	Acc@5 25.781 (21.210)
Epoch: [3][300/391]	Time 0.1903 Data 0.0022 Loss 7.7460 (4.2992)	Acc@1 3.125 (5.560)	Acc@5 25.000 (22.000)
Testing the models......
Loss: 4.3106, Prec@1: 7.40, Prec@5: 25.46
Epoch time: 82s
Saving models
Epoch: 4  lr: 0.100
Epoch: [4][000/391]	Time 0.9894 Data 0.7826 Loss 3.9992 (3.9992)	Acc@1 8.594 (8.594)	Acc@5 28.906 (28.906)
Epoch: [4][100/391]	Time 0.1883 Data 0.0020 Loss 4.2675 (4.1518)	Acc@1 7.812 (7.356)	Acc@5 30.469 (26.439)
Epoch: [4][200/391]	Time 0.1893 Data 0.0023 Loss 3.8366 (4.1377)	Acc@1 14.844 (8.182)	Acc@5 33.594 (27.884)
Epoch: [4][300/391]	Time 0.1885 Data 0.0021 Loss 3.6583 (4.1075)	Acc@1 12.500 (8.526)	Acc@5 35.156 (28.789)
Testing the models......
Loss: 3.9975, Prec@1: 11.06, Prec@5: 34.84
Epoch time: 81s
Saving models
Epoch: 5  lr: 0.100
Epoch: [5][000/391]	Time 1.2150 Data 1.0282 Loss 4.5573 (4.5573)	Acc@1 11.719 (11.719)	Acc@5 30.469 (30.469)
Epoch: [5][100/391]	Time 0.1895 Data 0.0021 Loss 3.7638 (3.9238)	Acc@1 12.500 (11.123)	Acc@5 39.062 (34.174)
Epoch: [5][200/391]	Time 0.1892 Data 0.0021 Loss 3.8452 (3.9095)	Acc@1 12.500 (12.014)	Acc@5 35.156 (35.020)
Epoch: [5][300/391]	Time 0.1893 Data 0.0022 Loss 3.7426 (3.8914)	Acc@1 17.188 (12.401)	Acc@5 35.938 (35.743)
Testing the models......
Loss: 3.6456, Prec@1: 17.05, Prec@5: 42.71
Epoch time: 82s
Saving models
Epoch: 6  lr: 0.100
Epoch: [6][000/391]	Time 1.1687 Data 0.9491 Loss 3.6619 (3.6619)	Acc@1 14.844 (14.844)	Acc@5 46.094 (46.094)
Epoch: [6][100/391]	Time 0.1894 Data 0.0020 Loss 3.6772 (3.6564)	Acc@1 10.938 (15.478)	Acc@5 51.562 (41.971)
Epoch: [6][200/391]	Time 0.1906 Data 0.0021 Loss 3.3778 (3.6445)	Acc@1 19.531 (16.126)	Acc@5 50.000 (42.685)
Epoch: [6][300/391]	Time 0.1892 Data 0.0021 Loss 3.4104 (3.6053)	Acc@1 23.438 (16.977)	Acc@5 46.094 (43.846)
Testing the models......
Loss: 3.3965, Prec@1: 21.37, Prec@5: 49.90
Epoch time: 82s
Saving models
Epoch: 7  lr: 0.100
Epoch: [7][000/391]	Time 1.2429 Data 0.9762 Loss 3.1588 (3.1588)	Acc@1 22.656 (22.656)	Acc@5 48.438 (48.438)
Epoch: [7][100/391]	Time 0.1896 Data 0.0022 Loss 3.1853 (3.4110)	Acc@1 21.094 (20.800)	Acc@5 49.219 (48.004)
Epoch: [7][200/391]	Time 0.1928 Data 0.0029 Loss 3.1439 (3.3734)	Acc@1 23.438 (21.226)	Acc@5 52.344 (49.122)
Epoch: [7][300/391]	Time 0.1890 Data 0.0021 Loss 3.0124 (3.3320)	Acc@1 25.781 (21.989)	Acc@5 57.812 (50.345)
Testing the models......
Loss: 3.3767, Prec@1: 25.11, Prec@5: 53.85
Epoch time: 82s
Saving models
Epoch: 8  lr: 0.100
Epoch: [8][000/391]	Time 1.2193 Data 0.9591 Loss 3.0788 (3.0788)	Acc@1 23.438 (23.438)	Acc@5 57.812 (57.812)
Epoch: [8][100/391]	Time 0.1947 Data 0.0021 Loss 3.1182 (3.1291)	Acc@1 29.688 (25.240)	Acc@5 50.000 (55.337)
Epoch: [8][200/391]	Time 0.1903 Data 0.0024 Loss 2.8451 (3.0984)	Acc@1 30.469 (25.828)	Acc@5 62.500 (55.815)
Epoch: [8][300/391]	Time 0.1898 Data 0.0021 Loss 2.9901 (3.0781)	Acc@1 20.312 (26.217)	Acc@5 55.469 (56.429)
Testing the models......
Loss: 3.0071, Prec@1: 28.81, Prec@5: 59.64
Epoch time: 82s
Saving models
Epoch: 9  lr: 0.100
Epoch: [9][000/391]	Time 1.2078 Data 0.9236 Loss 2.8690 (2.8690)	Acc@1 27.344 (27.344)	Acc@5 64.062 (64.062)
Epoch: [9][100/391]	Time 0.1916 Data 0.0021 Loss 2.5206 (2.8456)	Acc@1 35.938 (30.995)	Acc@5 69.531 (61.928)
Epoch: [9][200/391]	Time 0.1901 Data 0.0025 Loss 2.6961 (2.8181)	Acc@1 39.062 (31.203)	Acc@5 62.500 (62.337)
Epoch: [9][300/391]	Time 0.1903 Data 0.0020 Loss 2.6269 (2.7884)	Acc@1 34.375 (31.619)	Acc@5 67.188 (62.944)
Testing the models......
Loss: 2.5934, Prec@1: 35.57, Prec@5: 66.43
Epoch time: 82s
Saving models
Epoch: 10  lr: 0.100
Epoch: [10][000/391]	Time 1.1650 Data 0.9603 Loss 2.5007 (2.5007)	Acc@1 36.719 (36.719)	Acc@5 67.969 (67.969)
Epoch: [10][100/391]	Time 0.1899 Data 0.0021 Loss 2.0979 (2.5409)	Acc@1 42.969 (35.883)	Acc@5 73.438 (68.564)
Epoch: [10][200/391]	Time 0.1904 Data 0.0038 Loss 2.5330 (2.5270)	Acc@1 33.594 (36.346)	Acc@5 71.875 (68.571)
Epoch: [10][300/391]	Time 0.1925 Data 0.0034 Loss 2.4265 (2.5079)	Acc@1 37.500 (36.836)	Acc@5 66.406 (69.033)
Testing the models......
Loss: 2.5048, Prec@1: 37.31, Prec@5: 69.09
Epoch time: 82s
Saving models
Epoch: 11  lr: 0.100
Epoch: [11][000/391]	Time 1.2374 Data 1.0011 Loss 2.2219 (2.2219)	Acc@1 44.531 (44.531)	Acc@5 77.344 (77.344)
Epoch: [11][100/391]	Time 0.1900 Data 0.0020 Loss 2.5761 (2.2841)	Acc@1 34.375 (41.499)	Acc@5 67.969 (73.306)
Epoch: [11][200/391]	Time 0.1896 Data 0.0024 Loss 2.1904 (2.2842)	Acc@1 46.875 (41.325)	Acc@5 74.219 (73.368)
Epoch: [11][300/391]	Time 0.1900 Data 0.0019 Loss 2.2361 (2.2773)	Acc@1 46.875 (41.476)	Acc@5 73.438 (73.375)
Testing the models......
Loss: 2.3749, Prec@1: 40.74, Prec@5: 72.54
Epoch time: 82s
Saving models
Epoch: 12  lr: 0.100
Epoch: [12][000/391]	Time 1.2570 Data 1.0856 Loss 2.1862 (2.1862)	Acc@1 42.969 (42.969)	Acc@5 75.781 (75.781)
Epoch: [12][100/391]	Time 0.1902 Data 0.0020 Loss 2.2576 (2.1184)	Acc@1 42.969 (44.964)	Acc@5 73.438 (76.323)
Epoch: [12][200/391]	Time 0.1896 Data 0.0019 Loss 1.9684 (2.1068)	Acc@1 42.188 (45.002)	Acc@5 74.219 (76.796)
Epoch: [12][300/391]	Time 0.1902 Data 0.0025 Loss 2.0488 (2.0872)	Acc@1 42.969 (45.556)	Acc@5 74.219 (77.121)
Testing the models......
Loss: 2.3121, Prec@1: 42.93, Prec@5: 74.89
Epoch time: 82s
Saving models
Epoch: 13  lr: 0.100
Epoch: [13][000/391]	Time 1.1130 Data 0.8807 Loss 1.9420 (1.9420)	Acc@1 46.094 (46.094)	Acc@5 73.438 (73.438)
Epoch: [13][100/391]	Time 0.1898 Data 0.0020 Loss 2.1589 (1.9471)	Acc@1 40.625 (47.471)	Acc@5 75.000 (79.107)
Epoch: [13][200/391]	Time 0.1944 Data 0.0034 Loss 1.6453 (1.9164)	Acc@1 53.125 (47.769)	Acc@5 86.719 (79.656)
Epoch: [13][300/391]	Time 0.1894 Data 0.0020 Loss 2.1612 (1.8997)	Acc@1 38.281 (48.108)	Acc@5 70.312 (79.734)
Testing the models......
Loss: 1.9824, Prec@1: 46.92, Prec@5: 78.52
Epoch time: 82s
Saving models
Epoch: 14  lr: 0.100
Epoch: [14][000/391]	Time 1.1800 Data 0.9827 Loss 1.7615 (1.7615)	Acc@1 49.219 (49.219)	Acc@5 78.125 (78.125)
Epoch: [14][100/391]	Time 0.1890 Data 0.0020 Loss 1.7123 (1.7295)	Acc@1 52.344 (51.748)	Acc@5 81.250 (82.403)
Epoch: [14][200/391]	Time 0.1900 Data 0.0021 Loss 1.7675 (1.7477)	Acc@1 45.312 (51.531)	Acc@5 82.031 (82.222)
Epoch: [14][300/391]	Time 0.1898 Data 0.0021 Loss 1.6468 (1.7438)	Acc@1 55.469 (51.614)	Acc@5 82.812 (82.309)
Testing the models......
Loss: 1.8809, Prec@1: 49.59, Prec@5: 79.99
Epoch time: 82s
Saving models
Epoch: 15  lr: 0.100
Epoch: [15][000/391]	Time 1.1391 Data 0.9169 Loss 1.5884 (1.5884)	Acc@1 58.594 (58.594)	Acc@5 83.594 (83.594)
Epoch: [15][100/391]	Time 0.1925 Data 0.0020 Loss 1.5921 (1.5968)	Acc@1 54.688 (55.059)	Acc@5 88.281 (85.025)
Epoch: [15][200/391]	Time 0.1941 Data 0.0019 Loss 1.4721 (1.6139)	Acc@1 55.469 (54.629)	Acc@5 85.156 (84.756)
Epoch: [15][300/391]	Time 0.1898 Data 0.0022 Loss 2.1522 (1.6139)	Acc@1 43.750 (54.690)	Acc@5 74.219 (84.689)
Testing the models......
Loss: 1.7014, Prec@1: 53.80, Prec@5: 83.09
Epoch time: 82s
Saving models
Epoch: 16  lr: 0.100
Epoch: [16][000/391]	Time 1.1717 Data 0.9104 Loss 1.3625 (1.3625)	Acc@1 63.281 (63.281)	Acc@5 87.500 (87.500)
Epoch: [16][100/391]	Time 0.1883 Data 0.0019 Loss 1.5068 (1.4882)	Acc@1 59.375 (58.037)	Acc@5 85.156 (86.734)
Epoch: [16][200/391]	Time 0.1907 Data 0.0021 Loss 1.4360 (1.5027)	Acc@1 57.812 (57.789)	Acc@5 84.375 (86.353)
Epoch: [16][300/391]	Time 0.1973 Data 0.0020 Loss 1.4272 (1.5033)	Acc@1 59.375 (57.639)	Acc@5 89.844 (86.293)
Testing the models......
Loss: 1.8234, Prec@1: 51.87, Prec@5: 81.53
Epoch time: 82s
Epoch: 17  lr: 0.100
Epoch: [17][000/391]	Time 1.1696 Data 1.0087 Loss 1.3177 (1.3177)	Acc@1 60.156 (60.156)	Acc@5 88.281 (88.281)
Epoch: [17][100/391]	Time 0.1891 Data 0.0019 Loss 1.5958 (1.3951)	Acc@1 56.250 (59.669)	Acc@5 84.375 (87.794)
Epoch: [17][200/391]	Time 0.1890 Data 0.0019 Loss 1.4048 (1.4103)	Acc@1 54.688 (59.620)	Acc@5 89.062 (87.574)
Epoch: [17][300/391]	Time 0.1913 Data 0.0021 Loss 1.5915 (1.4186)	Acc@1 57.031 (59.526)	Acc@5 86.719 (87.575)
Testing the models......
Loss: 1.6454, Prec@1: 55.11, Prec@5: 83.68
Epoch time: 82s
Saving models
Epoch: 18  lr: 0.100
Epoch: [18][000/391]	Time 1.2954 Data 1.1256 Loss 1.2982 (1.2982)	Acc@1 64.062 (64.062)	Acc@5 85.938 (85.938)
Epoch: [18][100/391]	Time 0.1918 Data 0.0028 Loss 1.2814 (1.3161)	Acc@1 61.719 (61.982)	Acc@5 87.500 (88.807)
Epoch: [18][200/391]	Time 0.1906 Data 0.0030 Loss 1.4680 (1.3250)	Acc@1 60.156 (61.637)	Acc@5 85.938 (88.872)
Epoch: [18][300/391]	Time 0.1891 Data 0.0020 Loss 1.4129 (1.3315)	Acc@1 60.156 (61.695)	Acc@5 89.062 (88.803)
Testing the models......
Loss: 1.6284, Prec@1: 56.59, Prec@5: 84.30
Epoch time: 82s
Saving models
Epoch: 19  lr: 0.100
Epoch: [19][000/391]	Time 1.2182 Data 0.9306 Loss 1.2239 (1.2239)	Acc@1 62.500 (62.500)	Acc@5 92.188 (92.188)
Epoch: [19][100/391]	Time 0.1911 Data 0.0023 Loss 1.3612 (1.2234)	Acc@1 64.062 (64.333)	Acc@5 87.500 (90.695)
Epoch: [19][200/391]	Time 0.1892 Data 0.0031 Loss 1.4952 (1.2508)	Acc@1 61.719 (63.639)	Acc@5 85.156 (90.139)
Epoch: [19][300/391]	Time 0.1889 Data 0.0030 Loss 1.3302 (1.2604)	Acc@1 60.156 (63.437)	Acc@5 91.406 (89.999)
Testing the models......
Loss: 1.6201, Prec@1: 56.54, Prec@5: 84.43
Epoch time: 82s
Epoch: 20  lr: 0.100
Epoch: [20][000/391]	Time 1.1590 Data 0.9375 Loss 1.1773 (1.1773)	Acc@1 67.969 (67.969)	Acc@5 92.969 (92.969)
Epoch: [20][100/391]	Time 0.1930 Data 0.0058 Loss 1.0826 (1.1634)	Acc@1 67.188 (66.043)	Acc@5 92.188 (91.662)
Epoch: [20][200/391]	Time 0.1902 Data 0.0023 Loss 1.2294 (1.1649)	Acc@1 65.625 (66.243)	Acc@5 91.406 (91.305)
Epoch: [20][300/391]	Time 0.1892 Data 0.0020 Loss 1.2622 (1.1799)	Acc@1 67.188 (65.734)	Acc@5 89.844 (91.087)
Testing the models......
Loss: 1.4730, Prec@1: 59.85, Prec@5: 86.86
Epoch time: 82s
Saving models
Epoch: 21  lr: 0.100
Epoch: [21][000/391]	Time 1.1348 Data 0.9029 Loss 0.9450 (0.9450)	Acc@1 67.969 (67.969)	Acc@5 95.312 (95.312)
Epoch: [21][100/391]	Time 0.1897 Data 0.0039 Loss 0.9600 (1.0812)	Acc@1 71.094 (68.518)	Acc@5 93.750 (92.420)
Epoch: [21][200/391]	Time 0.1898 Data 0.0022 Loss 1.1609 (1.1041)	Acc@1 65.625 (67.631)	Acc@5 91.406 (92.094)
Epoch: [21][300/391]	Time 0.1895 Data 0.0020 Loss 1.0824 (1.1224)	Acc@1 68.750 (67.053)	Acc@5 93.750 (91.884)
Testing the models......
Loss: 1.4765, Prec@1: 59.61, Prec@5: 86.40
Epoch time: 82s
Epoch: 22  lr: 0.100
Epoch: [22][000/391]	Time 1.1663 Data 0.9705 Loss 1.1985 (1.1985)	Acc@1 63.281 (63.281)	Acc@5 92.188 (92.188)
Epoch: [22][100/391]	Time 0.1899 Data 0.0023 Loss 0.9689 (1.0259)	Acc@1 66.406 (69.005)	Acc@5 93.750 (93.108)
Epoch: [22][200/391]	Time 0.1894 Data 0.0024 Loss 1.0845 (1.0452)	Acc@1 67.188 (68.645)	Acc@5 88.281 (92.930)
Epoch: [22][300/391]	Time 0.1895 Data 0.0024 Loss 1.1691 (1.0622)	Acc@1 66.406 (68.278)	Acc@5 87.500 (92.605)
Testing the models......
Loss: 1.5107, Prec@1: 59.24, Prec@5: 86.55
Epoch time: 82s
Epoch: 23  lr: 0.100
Epoch: [23][000/391]	Time 1.1656 Data 0.9651 Loss 0.9695 (0.9695)	Acc@1 71.094 (71.094)	Acc@5 95.312 (95.312)
Epoch: [23][100/391]	Time 0.1917 Data 0.0024 Loss 0.8717 (0.9642)	Acc@1 73.438 (70.854)	Acc@5 93.750 (94.137)
Epoch: [23][200/391]	Time 0.1894 Data 0.0020 Loss 0.9458 (0.9895)	Acc@1 72.656 (70.449)	Acc@5 91.406 (93.598)
Epoch: [23][300/391]	Time 0.1909 Data 0.0024 Loss 1.0985 (1.0129)	Acc@1 64.844 (69.674)	Acc@5 92.969 (93.311)
Testing the models......
Loss: 1.5125, Prec@1: 59.77, Prec@5: 86.63
Epoch time: 82s
Epoch: 24  lr: 0.100
Epoch: [24][000/391]	Time 1.2121 Data 0.9912 Loss 1.0011 (1.0011)	Acc@1 65.625 (65.625)	Acc@5 92.188 (92.188)
Epoch: [24][100/391]	Time 0.1905 Data 0.0019 Loss 0.8317 (0.8980)	Acc@1 72.656 (73.120)	Acc@5 98.438 (94.601)
Epoch: [24][200/391]	Time 0.1900 Data 0.0020 Loss 0.9159 (0.9236)	Acc@1 69.531 (72.357)	Acc@5 94.531 (94.286)
Epoch: [24][300/391]	Time 0.1915 Data 0.0019 Loss 0.9106 (0.9524)	Acc@1 72.656 (71.545)	Acc@5 94.531 (94.020)
Testing the models......
Loss: 1.4277, Prec@1: 61.98, Prec@5: 87.35
Epoch time: 82s
Saving models
Epoch: 25  lr: 0.100
Epoch: [25][000/391]	Time 1.3078 Data 1.0873 Loss 0.7669 (0.7669)	Acc@1 78.125 (78.125)	Acc@5 95.312 (95.312)
Epoch: [25][100/391]	Time 0.1899 Data 0.0020 Loss 0.8708 (0.8646)	Acc@1 75.781 (73.847)	Acc@5 93.750 (95.088)
Epoch: [25][200/391]	Time 0.1895 Data 0.0022 Loss 1.1107 (0.9037)	Acc@1 75.781 (72.819)	Acc@5 92.188 (94.613)
Epoch: [25][300/391]	Time 0.1894 Data 0.0021 Loss 0.6756 (0.9116)	Acc@1 82.812 (72.734)	Acc@5 97.656 (94.521)
Testing the models......
Loss: 1.4995, Prec@1: 61.14, Prec@5: 87.28
Epoch time: 82s
Epoch: 26  lr: 0.100
Epoch: [26][000/391]	Time 1.1931 Data 0.9948 Loss 0.7782 (0.7782)	Acc@1 75.000 (75.000)	Acc@5 96.875 (96.875)
Epoch: [26][100/391]	Time 0.1910 Data 0.0021 Loss 0.7483 (0.8228)	Acc@1 68.750 (75.085)	Acc@5 98.438 (95.707)
Epoch: [26][200/391]	Time 0.1896 Data 0.0020 Loss 0.9907 (0.8546)	Acc@1 72.656 (74.067)	Acc@5 95.312 (95.176)
Epoch: [26][300/391]	Time 0.1906 Data 0.0020 Loss 1.0496 (0.8710)	Acc@1 70.312 (73.604)	Acc@5 94.531 (95.011)
Testing the models......
Loss: 1.4753, Prec@1: 61.76, Prec@5: 87.24
Epoch time: 82s
Epoch: 27  lr: 0.100
Epoch: [27][000/391]	Time 1.1383 Data 0.9743 Loss 0.6848 (0.6848)	Acc@1 76.562 (76.562)	Acc@5 97.656 (97.656)
Epoch: [27][100/391]	Time 0.1893 Data 0.0021 Loss 0.7792 (0.7818)	Acc@1 76.562 (75.704)	Acc@5 96.094 (95.978)
Epoch: [27][200/391]	Time 0.1938 Data 0.0019 Loss 0.7484 (0.8010)	Acc@1 75.000 (75.393)	Acc@5 97.656 (95.693)
Epoch: [27][300/391]	Time 0.1894 Data 0.0020 Loss 0.7096 (0.8303)	Acc@1 80.469 (74.595)	Acc@5 95.312 (95.323)
Testing the models......
Loss: 1.5013, Prec@1: 61.04, Prec@5: 86.66
Epoch time: 82s
Epoch: 28  lr: 0.100
Epoch: [28][000/391]	Time 1.1201 Data 0.9207 Loss 0.6547 (0.6547)	Acc@1 75.781 (75.781)	Acc@5 97.656 (97.656)
Epoch: [28][100/391]	Time 0.1904 Data 0.0020 Loss 0.6278 (0.7398)	Acc@1 78.906 (77.065)	Acc@5 98.438 (96.341)
Epoch: [28][200/391]	Time 0.1896 Data 0.0020 Loss 0.9044 (0.7692)	Acc@1 71.094 (76.360)	Acc@5 94.531 (96.051)
Epoch: [28][300/391]	Time 0.1912 Data 0.0022 Loss 0.9653 (0.7903)	Acc@1 69.531 (75.680)	Acc@5 94.531 (95.852)
Testing the models......
Loss: 1.5091, Prec@1: 61.00, Prec@5: 87.33
Epoch time: 82s
Epoch: 29  lr: 0.100
Epoch: [29][000/391]	Time 1.1331 Data 0.9434 Loss 0.8467 (0.8467)	Acc@1 70.312 (70.312)	Acc@5 95.312 (95.312)
Epoch: [29][100/391]	Time 0.1905 Data 0.0021 Loss 0.6656 (0.7119)	Acc@1 78.906 (77.645)	Acc@5 96.094 (96.798)
Epoch: [29][200/391]	Time 0.1891 Data 0.0021 Loss 0.7519 (0.7401)	Acc@1 76.562 (76.897)	Acc@5 92.969 (96.432)
Epoch: [29][300/391]	Time 0.1920 Data 0.0024 Loss 0.8397 (0.7603)	Acc@1 77.344 (76.544)	Acc@5 95.312 (96.143)
Testing the models......
Loss: 1.4627, Prec@1: 62.39, Prec@5: 87.47
Epoch time: 82s
Saving models
Epoch: 30  lr: 0.100
Epoch: [30][000/391]	Time 1.2670 Data 1.0323 Loss 0.7056 (0.7056)	Acc@1 79.688 (79.688)	Acc@5 96.875 (96.875)
Epoch: [30][100/391]	Time 0.1920 Data 0.0022 Loss 0.6237 (0.6823)	Acc@1 78.906 (79.053)	Acc@5 96.875 (97.123)
Epoch: [30][200/391]	Time 0.1918 Data 0.0023 Loss 0.6405 (0.7096)	Acc@1 79.688 (77.970)	Acc@5 97.656 (96.891)
Epoch: [30][300/391]	Time 0.1916 Data 0.0020 Loss 0.7721 (0.7324)	Acc@1 73.438 (77.323)	Acc@5 97.656 (96.615)
Testing the models......
Loss: 1.4509, Prec@1: 62.47, Prec@5: 88.04
Epoch time: 82s
Saving models
Epoch: 31  lr: 0.100
Epoch: [31][000/391]	Time 1.1963 Data 0.9963 Loss 0.6516 (0.6516)	Acc@1 77.344 (77.344)	Acc@5 96.875 (96.875)
Epoch: [31][100/391]	Time 0.1882 Data 0.0022 Loss 0.8440 (0.6345)	Acc@1 75.781 (80.306)	Acc@5 96.094 (97.471)
Epoch: [31][200/391]	Time 0.1896 Data 0.0023 Loss 0.6885 (0.6699)	Acc@1 78.125 (79.108)	Acc@5 98.438 (97.108)
Epoch: [31][300/391]	Time 0.1890 Data 0.0027 Loss 0.7388 (0.6930)	Acc@1 77.344 (78.520)	Acc@5 96.094 (96.828)
Testing the models......
Loss: 1.4126, Prec@1: 63.77, Prec@5: 88.25
Epoch time: 82s
Saving models
Epoch: 32  lr: 0.100
Epoch: [32][000/391]	Time 1.0633 Data 0.8596 Loss 0.6550 (0.6550)	Acc@1 81.250 (81.250)	Acc@5 95.312 (95.312)
Epoch: [32][100/391]	Time 0.1903 Data 0.0023 Loss 0.6419 (0.6284)	Acc@1 78.906 (80.523)	Acc@5 100.000 (97.370)
Epoch: [32][200/391]	Time 0.1924 Data 0.0024 Loss 0.5259 (0.6498)	Acc@1 86.719 (79.878)	Acc@5 98.438 (97.128)
Epoch: [32][300/391]	Time 0.1911 Data 0.0021 Loss 0.5939 (0.6738)	Acc@1 82.031 (79.148)	Acc@5 96.875 (96.878)
Testing the models......
Loss: 1.5161, Prec@1: 62.47, Prec@5: 87.20
Epoch time: 82s
Epoch: 33  lr: 0.100
Epoch: [33][000/391]	Time 1.0734 Data 0.9134 Loss 0.6082 (0.6082)	Acc@1 82.031 (82.031)	Acc@5 96.875 (96.875)
Epoch: [33][100/391]	Time 0.1903 Data 0.0028 Loss 0.7285 (0.6276)	Acc@1 76.562 (80.422)	Acc@5 96.875 (97.316)
Epoch: [33][200/391]	Time 0.1935 Data 0.0021 Loss 0.6151 (0.6342)	Acc@1 80.469 (80.243)	Acc@5 97.656 (97.330)
Epoch: [33][300/391]	Time 0.1906 Data 0.0022 Loss 0.6000 (0.6546)	Acc@1 79.688 (79.630)	Acc@5 98.438 (97.220)
Testing the models......
Loss: 1.4860, Prec@1: 63.15, Prec@5: 87.87
Epoch time: 82s
Epoch: 34  lr: 0.100
Epoch: [34][000/391]	Time 1.1107 Data 0.9164 Loss 0.6656 (0.6656)	Acc@1 78.125 (78.125)	Acc@5 96.094 (96.094)
Epoch: [34][100/391]	Time 0.1906 Data 0.0023 Loss 0.7311 (0.6015)	Acc@1 79.688 (81.258)	Acc@5 97.656 (97.679)
Epoch: [34][200/391]	Time 0.1898 Data 0.0020 Loss 0.6817 (0.6063)	Acc@1 79.688 (80.939)	Acc@5 98.438 (97.563)
Epoch: [34][300/391]	Time 0.1892 Data 0.0020 Loss 0.8446 (0.6257)	Acc@1 73.438 (80.357)	Acc@5 96.094 (97.446)
Testing the models......
Loss: 1.4044, Prec@1: 64.30, Prec@5: 88.93
Epoch time: 82s
Saving models
Epoch: 35  lr: 0.100
Epoch: [35][000/391]	Time 1.0605 Data 0.8432 Loss 0.4254 (0.4254)	Acc@1 86.719 (86.719)	Acc@5 99.219 (99.219)
Epoch: [35][100/391]	Time 0.1892 Data 0.0021 Loss 0.4730 (0.5500)	Acc@1 85.156 (82.673)	Acc@5 99.219 (98.167)
Epoch: [35][200/391]	Time 0.1889 Data 0.0020 Loss 0.8171 (0.5820)	Acc@1 72.656 (81.627)	Acc@5 96.094 (97.952)
Epoch: [35][300/391]	Time 0.1892 Data 0.0032 Loss 0.9376 (0.6038)	Acc@1 70.312 (81.050)	Acc@5 96.094 (97.607)
Testing the models......
Loss: 1.4555, Prec@1: 63.91, Prec@5: 88.01
Epoch time: 82s
Epoch: 36  lr: 0.100
Epoch: [36][000/391]	Time 1.1058 Data 0.9102 Loss 0.5242 (0.5242)	Acc@1 84.375 (84.375)	Acc@5 96.875 (96.875)
Epoch: [36][100/391]	Time 0.1891 Data 0.0020 Loss 0.5559 (0.5354)	Acc@1 81.250 (82.874)	Acc@5 96.875 (98.221)
Epoch: [36][200/391]	Time 0.1896 Data 0.0020 Loss 0.5112 (0.5577)	Acc@1 85.938 (82.148)	Acc@5 98.438 (98.010)
Epoch: [36][300/391]	Time 0.1893 Data 0.0020 Loss 0.5891 (0.5795)	Acc@1 82.812 (81.517)	Acc@5 96.094 (97.825)
Testing the models......
Loss: 1.4791, Prec@1: 63.39, Prec@5: 87.87
Epoch time: 82s
Epoch: 37  lr: 0.100
Epoch: [37][000/391]	Time 1.0552 Data 0.8563 Loss 0.4787 (0.4787)	Acc@1 84.375 (84.375)	Acc@5 100.000 (100.000)
Epoch: [37][100/391]	Time 0.1893 Data 0.0020 Loss 0.5730 (0.5187)	Acc@1 83.594 (83.478)	Acc@5 97.656 (98.352)
Epoch: [37][200/391]	Time 0.1895 Data 0.0020 Loss 0.5887 (0.5509)	Acc@1 80.469 (82.249)	Acc@5 97.656 (98.072)
Epoch: [37][300/391]	Time 0.1902 Data 0.0021 Loss 0.7106 (0.5706)	Acc@1 74.219 (81.774)	Acc@5 99.219 (97.950)
Testing the models......
Loss: 1.4920, Prec@1: 63.39, Prec@5: 87.88
Epoch time: 82s
Epoch: 38  lr: 0.100
Epoch: [38][000/391]	Time 1.0298 Data 0.8836 Loss 0.8078 (0.8078)	Acc@1 73.438 (73.438)	Acc@5 96.094 (96.094)
Epoch: [38][100/391]	Time 0.1919 Data 0.0022 Loss 0.3791 (0.5110)	Acc@1 85.938 (83.849)	Acc@5 98.438 (98.422)
Epoch: [38][200/391]	Time 0.1900 Data 0.0020 Loss 0.5578 (0.5301)	Acc@1 81.250 (83.042)	Acc@5 96.094 (98.266)
Epoch: [38][300/391]	Time 0.1901 Data 0.0051 Loss 0.8649 (0.5521)	Acc@1 75.000 (82.574)	Acc@5 93.750 (98.059)
Testing the models......
Loss: 1.4830, Prec@1: 64.01, Prec@5: 88.58
Epoch time: 82s
Epoch: 39  lr: 0.100
Epoch: [39][000/391]	Time 1.0959 Data 0.9070 Loss 0.5079 (0.5079)	Acc@1 82.031 (82.031)	Acc@5 99.219 (99.219)
Epoch: [39][100/391]	Time 0.1895 Data 0.0021 Loss 0.4873 (0.4590)	Acc@1 85.156 (85.435)	Acc@5 99.219 (98.786)
Epoch: [39][200/391]	Time 0.1902 Data 0.0021 Loss 0.5477 (0.4996)	Acc@1 84.375 (84.282)	Acc@5 100.000 (98.453)
Epoch: [39][300/391]	Time 0.1915 Data 0.0022 Loss 0.5722 (0.5213)	Acc@1 82.031 (83.544)	Acc@5 99.219 (98.295)
Testing the models......
Loss: 1.4564, Prec@1: 64.57, Prec@5: 88.14
Epoch time: 82s
Saving models
Epoch: 40  lr: 0.100
Epoch: [40][000/391]	Time 1.0982 Data 0.9164 Loss 0.4157 (0.4157)	Acc@1 83.594 (83.594)	Acc@5 99.219 (99.219)
Epoch: [40][100/391]	Time 0.1902 Data 0.0029 Loss 0.5277 (0.4666)	Acc@1 80.469 (85.365)	Acc@5 98.438 (98.646)
Epoch: [40][200/391]	Time 0.1905 Data 0.0025 Loss 0.5012 (0.4976)	Acc@1 82.031 (84.111)	Acc@5 97.656 (98.535)
Epoch: [40][300/391]	Time 0.1920 Data 0.0022 Loss 0.5614 (0.5168)	Acc@1 82.031 (83.646)	Acc@5 97.656 (98.297)
Testing the models......
Loss: 1.4462, Prec@1: 64.48, Prec@5: 88.35
Epoch time: 82s
Epoch: 41  lr: 0.100
Epoch: [41][000/391]	Time 1.1243 Data 0.9015 Loss 0.3831 (0.3831)	Acc@1 89.062 (89.062)	Acc@5 100.000 (100.000)
Epoch: [41][100/391]	Time 0.1894 Data 0.0021 Loss 0.4416 (0.4661)	Acc@1 85.938 (85.280)	Acc@5 99.219 (98.677)
Epoch: [41][200/391]	Time 0.1898 Data 0.0022 Loss 0.3408 (0.4820)	Acc@1 90.625 (84.884)	Acc@5 98.438 (98.566)
Epoch: [41][300/391]	Time 0.1905 Data 0.0021 Loss 0.7806 (0.5099)	Acc@1 75.781 (83.871)	Acc@5 96.875 (98.391)
Testing the models......
Loss: 1.4081, Prec@1: 64.66, Prec@5: 88.97
Epoch time: 82s
Saving models
Epoch: 42  lr: 0.100
Epoch: [42][000/391]	Time 1.0608 Data 0.7953 Loss 0.4483 (0.4483)	Acc@1 84.375 (84.375)	Acc@5 99.219 (99.219)
Epoch: [42][100/391]	Time 0.1917 Data 0.0021 Loss 0.4641 (0.4648)	Acc@1 86.719 (85.040)	Acc@5 98.438 (98.832)
Epoch: [42][200/391]	Time 0.1931 Data 0.0020 Loss 0.6083 (0.4772)	Acc@1 80.469 (84.869)	Acc@5 97.656 (98.698)
Epoch: [42][300/391]	Time 0.1918 Data 0.0025 Loss 0.6239 (0.4920)	Acc@1 81.250 (84.331)	Acc@5 96.875 (98.572)
Testing the models......
Loss: 1.5137, Prec@1: 64.72, Prec@5: 88.77
Epoch time: 81s
Saving models
Epoch: 43  lr: 0.100
Epoch: [43][000/391]	Time 1.0673 Data 0.8988 Loss 0.4040 (0.4040)	Acc@1 89.844 (89.844)	Acc@5 99.219 (99.219)
Epoch: [43][100/391]	Time 0.1886 Data 0.0024 Loss 0.4622 (0.4412)	Acc@1 86.719 (86.139)	Acc@5 100.000 (98.832)
Epoch: [43][200/391]	Time 0.1887 Data 0.0021 Loss 0.4317 (0.4687)	Acc@1 89.844 (85.180)	Acc@5 97.656 (98.694)
Epoch: [43][300/391]	Time 0.1907 Data 0.0022 Loss 0.5486 (0.4925)	Acc@1 81.250 (84.406)	Acc@5 97.656 (98.510)
Testing the models......
Loss: 1.4964, Prec@1: 64.40, Prec@5: 87.49
Epoch time: 81s
Epoch: 44  lr: 0.100
Epoch: [44][000/391]	Time 1.0484 Data 0.8277 Loss 0.3700 (0.3700)	Acc@1 87.500 (87.500)	Acc@5 100.000 (100.000)
Epoch: [44][100/391]	Time 0.1913 Data 0.0021 Loss 0.4228 (0.4371)	Acc@1 89.062 (86.162)	Acc@5 98.438 (98.871)
Epoch: [44][200/391]	Time 0.1899 Data 0.0027 Loss 0.4038 (0.4517)	Acc@1 88.281 (85.685)	Acc@5 100.000 (98.752)
Epoch: [44][300/391]	Time 0.1912 Data 0.0025 Loss 0.6248 (0.4742)	Acc@1 81.250 (84.998)	Acc@5 98.438 (98.715)
Testing the models......
Loss: 1.5127, Prec@1: 64.45, Prec@5: 88.00
Epoch time: 81s
Epoch: 45  lr: 0.100
Epoch: [45][000/391]	Time 1.0633 Data 0.8079 Loss 0.3256 (0.3256)	Acc@1 92.188 (92.188)	Acc@5 99.219 (99.219)
Epoch: [45][100/391]	Time 0.1891 Data 0.0021 Loss 0.3821 (0.4150)	Acc@1 86.719 (86.873)	Acc@5 99.219 (99.033)
Epoch: [45][200/391]	Time 0.1967 Data 0.0020 Loss 0.4335 (0.4343)	Acc@1 82.031 (86.089)	Acc@5 99.219 (98.881)
Epoch: [45][300/391]	Time 0.1891 Data 0.0020 Loss 0.5086 (0.4470)	Acc@1 83.594 (85.784)	Acc@5 99.219 (98.793)
Testing the models......
Loss: 1.4907, Prec@1: 64.72, Prec@5: 88.09
Epoch time: 81s
Epoch: 46  lr: 0.100
Epoch: [46][000/391]	Time 1.0388 Data 0.8776 Loss 0.3722 (0.3722)	Acc@1 87.500 (87.500)	Acc@5 99.219 (99.219)
Epoch: [46][100/391]	Time 0.1894 Data 0.0019 Loss 0.4924 (0.4265)	Acc@1 84.375 (86.634)	Acc@5 99.219 (98.817)
Epoch: [46][200/391]	Time 0.1892 Data 0.0021 Loss 0.2792 (0.4223)	Acc@1 91.406 (86.637)	Acc@5 100.000 (98.904)
Epoch: [46][300/391]	Time 0.1897 Data 0.0020 Loss 0.5595 (0.4431)	Acc@1 80.469 (86.023)	Acc@5 96.875 (98.824)
Testing the models......
Loss: 1.5003, Prec@1: 64.05, Prec@5: 88.50
Epoch time: 81s
Epoch: 47  lr: 0.100
Epoch: [47][000/391]	Time 1.0890 Data 0.8928 Loss 0.4137 (0.4137)	Acc@1 86.719 (86.719)	Acc@5 98.438 (98.438)
Epoch: [47][100/391]	Time 0.1892 Data 0.0022 Loss 0.2861 (0.3971)	Acc@1 91.406 (87.639)	Acc@5 100.000 (99.072)
Epoch: [47][200/391]	Time 0.1894 Data 0.0023 Loss 0.4874 (0.4161)	Acc@1 85.938 (86.983)	Acc@5 98.438 (98.916)
Epoch: [47][300/391]	Time 0.1901 Data 0.0030 Loss 0.6220 (0.4441)	Acc@1 77.344 (86.023)	Acc@5 95.312 (98.765)
Testing the models......
Loss: 1.4932, Prec@1: 64.66, Prec@5: 88.44
Epoch time: 81s
Epoch: 48  lr: 0.100
Epoch: [48][000/391]	Time 1.0863 Data 0.8863 Loss 0.4317 (0.4317)	Acc@1 86.719 (86.719)	Acc@5 99.219 (99.219)
Epoch: [48][100/391]	Time 0.1912 Data 0.0046 Loss 0.3265 (0.3984)	Acc@1 89.062 (87.469)	Acc@5 100.000 (99.002)
Epoch: [48][200/391]	Time 0.1893 Data 0.0022 Loss 0.4163 (0.4156)	Acc@1 88.281 (86.851)	Acc@5 99.219 (98.935)
Epoch: [48][300/391]	Time 0.1891 Data 0.0021 Loss 0.5764 (0.4287)	Acc@1 82.812 (86.532)	Acc@5 99.219 (98.881)
Testing the models......
Loss: 1.5327, Prec@1: 64.68, Prec@5: 88.34
Epoch time: 81s
Epoch: 49  lr: 0.100
Epoch: [49][000/391]	Time 1.0104 Data 0.8175 Loss 0.4330 (0.4330)	Acc@1 85.938 (85.938)	Acc@5 100.000 (100.000)
Epoch: [49][100/391]	Time 0.1887 Data 0.0021 Loss 0.1703 (0.3776)	Acc@1 93.750 (88.157)	Acc@5 100.000 (99.149)
Epoch: [49][200/391]	Time 0.1885 Data 0.0021 Loss 0.5645 (0.4012)	Acc@1 82.031 (87.411)	Acc@5 96.094 (99.032)
Epoch: [49][300/391]	Time 0.1916 Data 0.0020 Loss 0.4524 (0.4216)	Acc@1 83.594 (86.786)	Acc@5 99.219 (98.902)
Testing the models......
Loss: 1.5330, Prec@1: 64.38, Prec@5: 87.91
Epoch time: 81s
Epoch: 50  lr: 0.100
Epoch: [50][000/391]	Time 1.0216 Data 0.8241 Loss 0.3334 (0.3334)	Acc@1 92.969 (92.969)	Acc@5 98.438 (98.438)
Epoch: [50][100/391]	Time 0.1894 Data 0.0022 Loss 0.3032 (0.3748)	Acc@1 89.062 (88.065)	Acc@5 99.219 (99.250)
Epoch: [50][200/391]	Time 0.1896 Data 0.0021 Loss 0.4988 (0.3910)	Acc@1 82.812 (87.519)	Acc@5 98.438 (99.192)
Epoch: [50][300/391]	Time 0.1927 Data 0.0021 Loss 0.4904 (0.4101)	Acc@1 82.812 (86.900)	Acc@5 98.438 (99.050)
Testing the models......
Loss: 1.5404, Prec@1: 64.53, Prec@5: 87.38
Epoch time: 82s
Epoch: 51  lr: 0.100
Epoch: [51][000/391]	Time 0.9658 Data 0.8210 Loss 0.4010 (0.4010)	Acc@1 88.281 (88.281)	Acc@5 98.438 (98.438)
Epoch: [51][100/391]	Time 0.1920 Data 0.0023 Loss 0.2573 (0.3611)	Acc@1 91.406 (88.521)	Acc@5 99.219 (99.211)
Epoch: [51][200/391]	Time 0.1901 Data 0.0021 Loss 0.3728 (0.3644)	Acc@1 86.719 (88.402)	Acc@5 100.000 (99.238)
Epoch: [51][300/391]	Time 0.1894 Data 0.0020 Loss 0.3655 (0.3902)	Acc@1 90.625 (87.599)	Acc@5 100.000 (99.123)
Testing the models......
Loss: 1.5744, Prec@1: 64.23, Prec@5: 87.67
Epoch time: 82s
Epoch: 52  lr: 0.100
Epoch: [52][000/391]	Time 1.1012 Data 0.8342 Loss 0.3354 (0.3354)	Acc@1 92.969 (92.969)	Acc@5 98.438 (98.438)
Epoch: [52][100/391]	Time 0.1902 Data 0.0030 Loss 0.4452 (0.3604)	Acc@1 89.062 (88.444)	Acc@5 99.219 (99.312)
Epoch: [52][200/391]	Time 0.1906 Data 0.0025 Loss 0.4927 (0.3773)	Acc@1 86.719 (87.924)	Acc@5 100.000 (99.254)
Epoch: [52][300/391]	Time 0.1918 Data 0.0023 Loss 0.4090 (0.4028)	Acc@1 85.938 (87.054)	Acc@5 100.000 (99.141)
Testing the models......
Loss: 1.5455, Prec@1: 64.11, Prec@5: 88.20
Epoch time: 82s
Epoch: 53  lr: 0.100
Epoch: [53][000/391]	Time 0.9856 Data 0.7632 Loss 0.4816 (0.4816)	Acc@1 85.156 (85.156)	Acc@5 97.656 (97.656)
Epoch: [53][100/391]	Time 0.1912 Data 0.0025 Loss 0.3295 (0.3733)	Acc@1 88.281 (88.034)	Acc@5 100.000 (99.296)
Epoch: [53][200/391]	Time 0.1899 Data 0.0020 Loss 0.2983 (0.3828)	Acc@1 87.500 (87.698)	Acc@5 100.000 (99.168)
Epoch: [53][300/391]	Time 0.1899 Data 0.0021 Loss 0.3664 (0.3986)	Acc@1 89.844 (87.233)	Acc@5 98.438 (99.079)
Testing the models......
Loss: 1.4613, Prec@1: 65.76, Prec@5: 89.00
Epoch time: 82s
Saving models
Epoch: 54  lr: 0.100
Epoch: [54][000/391]	Time 0.9856 Data 0.7156 Loss 0.2564 (0.2564)	Acc@1 92.188 (92.188)	Acc@5 99.219 (99.219)
Epoch: [54][100/391]	Time 0.1905 Data 0.0026 Loss 0.3740 (0.3442)	Acc@1 89.844 (89.295)	Acc@5 99.219 (99.335)
Epoch: [54][200/391]	Time 0.1934 Data 0.0027 Loss 0.3463 (0.3549)	Acc@1 88.281 (88.806)	Acc@5 98.438 (99.238)
Epoch: [54][300/391]	Time 0.1921 Data 0.0022 Loss 0.4550 (0.3754)	Acc@1 84.375 (88.190)	Acc@5 98.438 (99.156)
Testing the models......
Loss: 1.6351, Prec@1: 62.68, Prec@5: 86.55
Epoch time: 82s
Epoch: 55  lr: 0.100
Epoch: [55][000/391]	Time 0.9877 Data 0.8273 Loss 0.3580 (0.3580)	Acc@1 89.062 (89.062)	Acc@5 98.438 (98.438)
Epoch: [55][100/391]	Time 0.1923 Data 0.0021 Loss 0.3405 (0.3366)	Acc@1 89.844 (89.542)	Acc@5 100.000 (99.250)
Epoch: [55][200/391]	Time 0.1897 Data 0.0021 Loss 0.6202 (0.3658)	Acc@1 77.344 (88.421)	Acc@5 98.438 (99.234)
Epoch: [55][300/391]	Time 0.1903 Data 0.0020 Loss 0.3091 (0.3861)	Acc@1 89.062 (87.760)	Acc@5 100.000 (99.125)
Testing the models......
Loss: 1.6298, Prec@1: 63.22, Prec@5: 86.77
Epoch time: 82s
Epoch: 56  lr: 0.100
Epoch: [56][000/391]	Time 0.9853 Data 0.7796 Loss 0.3441 (0.3441)	Acc@1 89.062 (89.062)	Acc@5 100.000 (100.000)
Epoch: [56][100/391]	Time 0.1901 Data 0.0020 Loss 0.3094 (0.3535)	Acc@1 89.062 (88.683)	Acc@5 100.000 (99.281)
Epoch: [56][200/391]	Time 0.1891 Data 0.0021 Loss 0.5787 (0.3646)	Acc@1 82.031 (88.429)	Acc@5 97.656 (99.258)
Epoch: [56][300/391]	Time 0.1901 Data 0.0021 Loss 0.3940 (0.3772)	Acc@1 87.500 (88.006)	Acc@5 100.000 (99.159)
Testing the models......
Loss: 1.4871, Prec@1: 65.72, Prec@5: 88.79
Epoch time: 82s
Epoch: 57  lr: 0.100
Epoch: [57][000/391]	Time 1.0356 Data 0.8336 Loss 0.3918 (0.3918)	Acc@1 89.844 (89.844)	Acc@5 99.219 (99.219)
Epoch: [57][100/391]	Time 0.1898 Data 0.0022 Loss 0.3252 (0.3253)	Acc@1 89.062 (89.596)	Acc@5 98.438 (99.420)
Epoch: [57][200/391]	Time 0.1901 Data 0.0020 Loss 0.3174 (0.3435)	Acc@1 86.719 (89.129)	Acc@5 100.000 (99.324)
Epoch: [57][300/391]	Time 0.1902 Data 0.0031 Loss 0.3859 (0.3648)	Acc@1 89.062 (88.411)	Acc@5 100.000 (99.252)
Testing the models......
Loss: 1.5759, Prec@1: 64.31, Prec@5: 88.13
Epoch time: 82s
Epoch: 58  lr: 0.100
Epoch: [58][000/391]	Time 1.0088 Data 0.8491 Loss 0.2704 (0.2704)	Acc@1 90.625 (90.625)	Acc@5 99.219 (99.219)
Epoch: [58][100/391]	Time 0.1921 Data 0.0022 Loss 0.5106 (0.3487)	Acc@1 84.375 (88.792)	Acc@5 99.219 (99.389)
Epoch: [58][200/391]	Time 0.1885 Data 0.0019 Loss 0.3964 (0.3517)	Acc@1 88.281 (88.822)	Acc@5 99.219 (99.328)
Epoch: [58][300/391]	Time 0.1893 Data 0.0020 Loss 0.3361 (0.3717)	Acc@1 90.625 (88.190)	Acc@5 99.219 (99.227)
Testing the models......
Loss: 1.5473, Prec@1: 65.11, Prec@5: 88.01
Epoch time: 81s
Epoch: 59  lr: 0.100
Epoch: [59][000/391]	Time 0.9633 Data 0.7735 Loss 0.4271 (0.4271)	Acc@1 88.281 (88.281)	Acc@5 99.219 (99.219)
Epoch: [59][100/391]	Time 0.1892 Data 0.0021 Loss 0.2412 (0.3279)	Acc@1 92.188 (89.705)	Acc@5 99.219 (99.381)
Epoch: [59][200/391]	Time 0.1899 Data 0.0022 Loss 0.4278 (0.3397)	Acc@1 83.594 (89.292)	Acc@5 98.438 (99.320)
Epoch: [59][300/391]	Time 0.1890 Data 0.0021 Loss 0.5060 (0.3572)	Acc@1 83.594 (88.684)	Acc@5 99.219 (99.271)
Testing the models......
Loss: 1.5301, Prec@1: 64.63, Prec@5: 88.80
Epoch time: 81s
Epoch: 60  lr: 0.100
Epoch: [60][000/391]	Time 1.0239 Data 0.8310 Loss 0.3127 (0.3127)	Acc@1 89.844 (89.844)	Acc@5 100.000 (100.000)
Epoch: [60][100/391]	Time 0.1910 Data 0.0022 Loss 0.3746 (0.3493)	Acc@1 90.625 (88.970)	Acc@5 98.438 (99.373)
Epoch: [60][200/391]	Time 0.1885 Data 0.0020 Loss 0.3747 (0.3547)	Acc@1 89.844 (88.701)	Acc@5 98.438 (99.355)
Epoch: [60][300/391]	Time 0.1883 Data 0.0020 Loss 0.3680 (0.3721)	Acc@1 89.062 (88.320)	Acc@5 98.438 (99.245)
Testing the models......
Loss: 1.5732, Prec@1: 64.66, Prec@5: 87.28
Epoch time: 81s
Epoch: 61  lr: 0.100
Epoch: [61][000/391]	Time 1.0162 Data 0.8168 Loss 0.4593 (0.4593)	Acc@1 87.500 (87.500)	Acc@5 97.656 (97.656)
Epoch: [61][100/391]	Time 0.1885 Data 0.0020 Loss 0.3344 (0.3373)	Acc@1 89.844 (89.519)	Acc@5 100.000 (99.319)
Epoch: [61][200/391]	Time 0.1890 Data 0.0020 Loss 0.4019 (0.3454)	Acc@1 85.938 (89.051)	Acc@5 99.219 (99.331)
Epoch: [61][300/391]	Time 0.1892 Data 0.0022 Loss 0.4724 (0.3596)	Acc@1 88.281 (88.629)	Acc@5 96.875 (99.224)
Testing the models......
Loss: 1.4689, Prec@1: 66.86, Prec@5: 89.02
Epoch time: 81s
Saving models
Epoch: 62  lr: 0.100
Epoch: [62][000/391]	Time 1.0587 Data 0.8280 Loss 0.2363 (0.2363)	Acc@1 92.969 (92.969)	Acc@5 100.000 (100.000)
Epoch: [62][100/391]	Time 0.1901 Data 0.0020 Loss 0.1873 (0.3099)	Acc@1 96.875 (90.285)	Acc@5 100.000 (99.482)
Epoch: [62][200/391]	Time 0.1909 Data 0.0022 Loss 0.3806 (0.3287)	Acc@1 85.938 (89.634)	Acc@5 98.438 (99.417)
Epoch: [62][300/391]	Time 0.1889 Data 0.0022 Loss 0.3528 (0.3499)	Acc@1 88.281 (88.917)	Acc@5 99.219 (99.252)
Testing the models......
Loss: 1.5316, Prec@1: 65.89, Prec@5: 88.33
Epoch time: 81s
Epoch: 63  lr: 0.100
Epoch: [63][000/391]	Time 0.9922 Data 0.7936 Loss 0.4028 (0.4028)	Acc@1 88.281 (88.281)	Acc@5 98.438 (98.438)
Epoch: [63][100/391]	Time 0.1892 Data 0.0022 Loss 0.3554 (0.3065)	Acc@1 88.281 (90.370)	Acc@5 99.219 (99.389)
Epoch: [63][200/391]	Time 0.1894 Data 0.0022 Loss 0.4333 (0.3066)	Acc@1 87.500 (90.392)	Acc@5 100.000 (99.390)
Epoch: [63][300/391]	Time 0.1892 Data 0.0020 Loss 0.3268 (0.3254)	Acc@1 87.500 (89.761)	Acc@5 100.000 (99.369)
Testing the models......
Loss: 1.6173, Prec@1: 64.34, Prec@5: 87.80
Epoch time: 81s
Epoch: 64  lr: 0.100
Epoch: [64][000/391]	Time 1.0083 Data 0.8320 Loss 0.3878 (0.3878)	Acc@1 88.281 (88.281)	Acc@5 99.219 (99.219)
Epoch: [64][100/391]	Time 0.1897 Data 0.0021 Loss 0.2519 (0.3290)	Acc@1 95.312 (89.596)	Acc@5 100.000 (99.389)
Epoch: [64][200/391]	Time 0.1888 Data 0.0020 Loss 0.4799 (0.3476)	Acc@1 83.594 (88.958)	Acc@5 100.000 (99.285)
Epoch: [64][300/391]	Time 0.1890 Data 0.0021 Loss 0.3424 (0.3629)	Acc@1 91.406 (88.525)	Acc@5 99.219 (99.198)
Testing the models......
Loss: 1.5089, Prec@1: 65.63, Prec@5: 88.81
Epoch time: 81s
Epoch: 65  lr: 0.100
Epoch: [65][000/391]	Time 0.9921 Data 0.8416 Loss 0.2731 (0.2731)	Acc@1 92.188 (92.188)	Acc@5 99.219 (99.219)
Epoch: [65][100/391]	Time 0.1895 Data 0.0021 Loss 0.3526 (0.3242)	Acc@1 88.281 (89.511)	Acc@5 99.219 (99.327)
Epoch: [65][200/391]	Time 0.1897 Data 0.0021 Loss 0.4238 (0.3338)	Acc@1 87.500 (89.424)	Acc@5 99.219 (99.289)
Epoch: [65][300/391]	Time 0.1915 Data 0.0021 Loss 0.3202 (0.3540)	Acc@1 86.719 (88.839)	Acc@5 100.000 (99.232)
Testing the models......
Loss: 1.7060, Prec@1: 63.05, Prec@5: 86.93
Epoch time: 82s
Epoch: 66  lr: 0.100
Epoch: [66][000/391]	Time 0.9808 Data 0.8314 Loss 0.3365 (0.3365)	Acc@1 85.156 (85.156)	Acc@5 100.000 (100.000)
Epoch: [66][100/391]	Time 0.1955 Data 0.0022 Loss 0.3843 (0.3361)	Acc@1 86.719 (89.387)	Acc@5 100.000 (99.242)
Epoch: [66][200/391]	Time 0.1924 Data 0.0021 Loss 0.5526 (0.3438)	Acc@1 85.156 (89.086)	Acc@5 96.875 (99.219)
Epoch: [66][300/391]	Time 0.1910 Data 0.0026 Loss 0.4960 (0.3559)	Acc@1 85.938 (88.733)	Acc@5 98.438 (99.172)
Testing the models......
Loss: 1.5892, Prec@1: 64.69, Prec@5: 88.01
Epoch time: 82s
Epoch: 67  lr: 0.100
Epoch: [67][000/391]	Time 1.0398 Data 0.8163 Loss 0.2333 (0.2333)	Acc@1 91.406 (91.406)	Acc@5 99.219 (99.219)
Epoch: [67][100/391]	Time 0.1914 Data 0.0031 Loss 0.3494 (0.3242)	Acc@1 88.281 (89.596)	Acc@5 98.438 (99.281)
Epoch: [67][200/391]	Time 0.1909 Data 0.0020 Loss 0.1732 (0.3376)	Acc@1 94.531 (89.307)	Acc@5 100.000 (99.281)
Epoch: [67][300/391]	Time 0.1903 Data 0.0030 Loss 0.3800 (0.3449)	Acc@1 86.719 (89.005)	Acc@5 100.000 (99.250)
Testing the models......
Loss: 1.5175, Prec@1: 65.97, Prec@5: 88.34
Epoch time: 82s
Epoch: 68  lr: 0.100
Epoch: [68][000/391]	Time 1.0940 Data 0.9127 Loss 0.2927 (0.2927)	Acc@1 90.625 (90.625)	Acc@5 99.219 (99.219)
Epoch: [68][100/391]	Time 0.1905 Data 0.0021 Loss 0.3165 (0.2929)	Acc@1 92.188 (91.120)	Acc@5 100.000 (99.551)
Epoch: [68][200/391]	Time 0.1936 Data 0.0021 Loss 0.3199 (0.2989)	Acc@1 88.281 (90.870)	Acc@5 100.000 (99.460)
Epoch: [68][300/391]	Time 0.1919 Data 0.0021 Loss 0.3805 (0.3144)	Acc@1 88.281 (90.269)	Acc@5 98.438 (99.390)
Testing the models......
Loss: 1.6254, Prec@1: 63.67, Prec@5: 87.14
Epoch time: 82s
Epoch: 69  lr: 0.100
Epoch: [69][000/391]	Time 0.9287 Data 0.7155 Loss 0.3965 (0.3965)	Acc@1 87.500 (87.500)	Acc@5 99.219 (99.219)
Epoch: [69][100/391]	Time 0.1898 Data 0.0020 Loss 0.2131 (0.2952)	Acc@1 95.312 (90.733)	Acc@5 100.000 (99.520)
Epoch: [69][200/391]	Time 0.1919 Data 0.0020 Loss 0.3736 (0.3068)	Acc@1 89.062 (90.353)	Acc@5 99.219 (99.460)
Epoch: [69][300/391]	Time 0.1892 Data 0.0020 Loss 0.3498 (0.3339)	Acc@1 87.500 (89.480)	Acc@5 100.000 (99.385)
Testing the models......
Loss: 1.6227, Prec@1: 64.65, Prec@5: 87.00
Epoch time: 81s
Epoch: 70  lr: 0.100
Epoch: [70][000/391]	Time 0.9772 Data 0.7849 Loss 0.3126 (0.3126)	Acc@1 88.281 (88.281)	Acc@5 100.000 (100.000)
Epoch: [70][100/391]	Time 0.1931 Data 0.0021 Loss 0.3371 (0.2805)	Acc@1 91.406 (91.089)	Acc@5 100.000 (99.613)
Epoch: [70][200/391]	Time 0.1908 Data 0.0021 Loss 0.4125 (0.2870)	Acc@1 87.500 (91.014)	Acc@5 99.219 (99.588)
Epoch: [70][300/391]	Time 0.1895 Data 0.0021 Loss 0.3044 (0.3036)	Acc@1 90.625 (90.516)	Acc@5 99.219 (99.525)
Testing the models......
Loss: 1.6815, Prec@1: 63.48, Prec@5: 87.26
Epoch time: 81s
Epoch: 71  lr: 0.100
Epoch: [71][000/391]	Time 0.9534 Data 0.7393 Loss 0.3085 (0.3085)	Acc@1 91.406 (91.406)	Acc@5 100.000 (100.000)
Epoch: [71][100/391]	Time 0.1890 Data 0.0020 Loss 0.2880 (0.3201)	Acc@1 91.406 (89.735)	Acc@5 99.219 (99.381)
Epoch: [71][200/391]	Time 0.1898 Data 0.0019 Loss 0.2861 (0.3140)	Acc@1 89.844 (89.879)	Acc@5 100.000 (99.464)
Epoch: [71][300/391]	Time 0.1892 Data 0.0020 Loss 0.3436 (0.3306)	Acc@1 89.844 (89.439)	Acc@5 100.000 (99.374)
Testing the models......
Loss: 1.5867, Prec@1: 64.79, Prec@5: 87.59
Epoch time: 81s
Epoch: 72  lr: 0.100
Epoch: [72][000/391]	Time 0.9899 Data 0.8417 Loss 0.4557 (0.4557)	Acc@1 84.375 (84.375)	Acc@5 100.000 (100.000)
Epoch: [72][100/391]	Time 0.1891 Data 0.0020 Loss 0.4493 (0.3122)	Acc@1 85.156 (90.393)	Acc@5 99.219 (99.412)
Epoch: [72][200/391]	Time 0.1901 Data 0.0022 Loss 0.2650 (0.3167)	Acc@1 92.188 (90.120)	Acc@5 99.219 (99.436)
Epoch: [72][300/391]	Time 0.1912 Data 0.0019 Loss 0.3015 (0.3312)	Acc@1 91.406 (89.634)	Acc@5 100.000 (99.349)
Testing the models......
Loss: 1.4854, Prec@1: 66.47, Prec@5: 88.81
Epoch time: 81s
Epoch: 73  lr: 0.100
Epoch: [73][000/391]	Time 1.2478 Data 1.0394 Loss 0.3937 (0.3937)	Acc@1 88.281 (88.281)	Acc@5 97.656 (97.656)
Epoch: [73][100/391]	Time 0.1892 Data 0.0022 Loss 0.3120 (0.2776)	Acc@1 90.625 (91.352)	Acc@5 100.000 (99.497)
Epoch: [73][200/391]	Time 0.1888 Data 0.0022 Loss 0.3253 (0.2911)	Acc@1 91.406 (90.784)	Acc@5 99.219 (99.475)
Epoch: [73][300/391]	Time 0.1888 Data 0.0025 Loss 0.2146 (0.3152)	Acc@1 96.875 (89.997)	Acc@5 100.000 (99.408)
Testing the models......
Loss: 1.7414, Prec@1: 62.87, Prec@5: 86.61
Epoch time: 81s
Epoch: 74  lr: 0.100
Epoch: [74][000/391]	Time 1.1689 Data 0.9688 Loss 0.3053 (0.3053)	Acc@1 91.406 (91.406)	Acc@5 100.000 (100.000)
Epoch: [74][100/391]	Time 0.1885 Data 0.0020 Loss 0.3258 (0.2869)	Acc@1 89.844 (90.989)	Acc@5 99.219 (99.381)
Epoch: [74][200/391]	Time 0.1911 Data 0.0022 Loss 0.4031 (0.3016)	Acc@1 85.938 (90.427)	Acc@5 100.000 (99.433)
Epoch: [74][300/391]	Time 0.1943 Data 0.0022 Loss 0.3422 (0.3252)	Acc@1 87.500 (89.758)	Acc@5 99.219 (99.297)
Testing the models......
Loss: 1.6212, Prec@1: 65.05, Prec@5: 88.07
Epoch time: 82s
Epoch: 75  lr: 0.100
Epoch: [75][000/391]	Time 1.0566 Data 0.8564 Loss 0.2705 (0.2705)	Acc@1 92.188 (92.188)	Acc@5 100.000 (100.000)
Epoch: [75][100/391]	Time 0.1890 Data 0.0022 Loss 0.1798 (0.2918)	Acc@1 95.312 (90.772)	Acc@5 100.000 (99.528)
Epoch: [75][200/391]	Time 0.1917 Data 0.0023 Loss 0.3489 (0.2998)	Acc@1 90.625 (90.516)	Acc@5 100.000 (99.495)
Epoch: [75][300/391]	Time 0.1897 Data 0.0021 Loss 0.5516 (0.3162)	Acc@1 81.250 (90.038)	Acc@5 97.656 (99.442)
Testing the models......
Loss: 1.6607, Prec@1: 64.58, Prec@5: 87.35
Epoch time: 81s
Epoch: 76  lr: 0.100
Epoch: [76][000/391]	Time 1.0386 Data 0.8821 Loss 0.2632 (0.2632)	Acc@1 92.188 (92.188)	Acc@5 99.219 (99.219)
Epoch: [76][100/391]	Time 0.1878 Data 0.0021 Loss 0.3762 (0.3010)	Acc@1 88.281 (90.501)	Acc@5 100.000 (99.459)
Epoch: [76][200/391]	Time 0.1904 Data 0.0023 Loss 0.3479 (0.3027)	Acc@1 88.281 (90.392)	Acc@5 99.219 (99.491)
Epoch: [76][300/391]	Time 0.1905 Data 0.0021 Loss 0.4344 (0.3185)	Acc@1 85.938 (89.935)	Acc@5 99.219 (99.429)
Testing the models......
Loss: 1.5763, Prec@1: 65.12, Prec@5: 87.77
Epoch time: 81s
Epoch: 77  lr: 0.100
Epoch: [77][000/391]	Time 1.0276 Data 0.8253 Loss 0.2792 (0.2792)	Acc@1 88.281 (88.281)	Acc@5 99.219 (99.219)
Epoch: [77][100/391]	Time 0.1892 Data 0.0020 Loss 0.1585 (0.2905)	Acc@1 94.531 (90.702)	Acc@5 100.000 (99.551)
Epoch: [77][200/391]	Time 0.1897 Data 0.0025 Loss 0.2653 (0.2984)	Acc@1 88.281 (90.470)	Acc@5 100.000 (99.479)
Epoch: [77][300/391]	Time 0.1895 Data 0.0020 Loss 0.3354 (0.3121)	Acc@1 88.281 (90.147)	Acc@5 100.000 (99.395)
Testing the models......
Loss: 1.6166, Prec@1: 64.81, Prec@5: 87.68
Epoch time: 81s
Epoch: 78  lr: 0.100
Epoch: [78][000/391]	Time 0.9842 Data 0.7851 Loss 0.2952 (0.2952)	Acc@1 92.188 (92.188)	Acc@5 99.219 (99.219)
Epoch: [78][100/391]	Time 0.1894 Data 0.0021 Loss 0.2780 (0.2773)	Acc@1 91.406 (91.151)	Acc@5 98.438 (99.489)
Epoch: [78][200/391]	Time 0.1893 Data 0.0020 Loss 0.2697 (0.2905)	Acc@1 89.062 (90.827)	Acc@5 100.000 (99.499)
Epoch: [78][300/391]	Time 0.1892 Data 0.0024 Loss 0.3738 (0.3055)	Acc@1 88.281 (90.464)	Acc@5 98.438 (99.437)
Testing the models......
Loss: 1.5210, Prec@1: 66.20, Prec@5: 88.34
Epoch time: 81s
Epoch: 79  lr: 0.100
Epoch: [79][000/391]	Time 1.0515 Data 0.8936 Loss 0.2524 (0.2524)	Acc@1 90.625 (90.625)	Acc@5 98.438 (98.438)
Epoch: [79][100/391]	Time 0.1888 Data 0.0021 Loss 0.2545 (0.2738)	Acc@1 95.312 (91.576)	Acc@5 99.219 (99.613)
Epoch: [79][200/391]	Time 0.1913 Data 0.0021 Loss 0.4180 (0.2974)	Acc@1 87.500 (90.652)	Acc@5 100.000 (99.464)
Epoch: [79][300/391]	Time 0.1904 Data 0.0021 Loss 0.4497 (0.3207)	Acc@1 85.156 (90.007)	Acc@5 98.438 (99.400)
Testing the models......
Loss: 1.6306, Prec@1: 64.47, Prec@5: 87.92
Epoch time: 82s
Epoch: 80  lr: 0.100
Epoch: [80][000/391]	Time 1.0749 Data 0.8594 Loss 0.2206 (0.2206)	Acc@1 92.969 (92.969)	Acc@5 100.000 (100.000)
Epoch: [80][100/391]	Time 0.1891 Data 0.0022 Loss 0.2210 (0.2877)	Acc@1 92.969 (91.151)	Acc@5 100.000 (99.466)
Epoch: [80][200/391]	Time 0.1895 Data 0.0021 Loss 0.2854 (0.2887)	Acc@1 92.969 (90.998)	Acc@5 100.000 (99.495)
Epoch: [80][300/391]	Time 0.1890 Data 0.0020 Loss 0.2796 (0.3038)	Acc@1 90.625 (90.558)	Acc@5 100.000 (99.447)
Testing the models......
Loss: 1.6210, Prec@1: 65.02, Prec@5: 87.45
Epoch time: 81s
Epoch: 81  lr: 0.100
Epoch: [81][000/391]	Time 1.0471 Data 0.7899 Loss 0.2032 (0.2032)	Acc@1 95.312 (95.312)	Acc@5 100.000 (100.000)
Epoch: [81][100/391]	Time 0.1892 Data 0.0021 Loss 0.2748 (0.2700)	Acc@1 90.625 (91.654)	Acc@5 99.219 (99.513)
Epoch: [81][200/391]	Time 0.1899 Data 0.0022 Loss 0.2276 (0.2806)	Acc@1 90.625 (91.278)	Acc@5 100.000 (99.534)
Epoch: [81][300/391]	Time 0.1908 Data 0.0028 Loss 0.3407 (0.2978)	Acc@1 88.281 (90.648)	Acc@5 100.000 (99.471)
Testing the models......
Loss: 1.6654, Prec@1: 64.28, Prec@5: 87.08
Epoch time: 82s
Epoch: 82  lr: 0.100
Epoch: [82][000/391]	Time 1.0451 Data 0.7754 Loss 0.2560 (0.2560)	Acc@1 91.406 (91.406)	Acc@5 100.000 (100.000)
Epoch: [82][100/391]	Time 0.1926 Data 0.0024 Loss 0.5093 (0.2771)	Acc@1 81.250 (91.375)	Acc@5 97.656 (99.482)
Epoch: [82][200/391]	Time 0.1915 Data 0.0028 Loss 0.3088 (0.2780)	Acc@1 91.406 (91.297)	Acc@5 100.000 (99.545)
Epoch: [82][300/391]	Time 0.1905 Data 0.0024 Loss 0.3486 (0.2924)	Acc@1 89.062 (90.916)	Acc@5 100.000 (99.507)
Testing the models......
Loss: 1.7465, Prec@1: 63.61, Prec@5: 87.01
Epoch time: 82s
Epoch: 83  lr: 0.100
Epoch: [83][000/391]	Time 1.0082 Data 0.7916 Loss 0.2610 (0.2610)	Acc@1 91.406 (91.406)	Acc@5 100.000 (100.000)
Epoch: [83][100/391]	Time 0.1890 Data 0.0020 Loss 0.1735 (0.2979)	Acc@1 93.750 (90.679)	Acc@5 100.000 (99.443)
Epoch: [83][200/391]	Time 0.1919 Data 0.0023 Loss 0.1780 (0.3011)	Acc@1 93.750 (90.497)	Acc@5 100.000 (99.468)
Epoch: [83][300/391]	Time 0.1909 Data 0.0032 Loss 0.3310 (0.3120)	Acc@1 90.625 (90.189)	Acc@5 100.000 (99.426)
Testing the models......
Loss: 1.6355, Prec@1: 64.81, Prec@5: 87.58
Epoch time: 82s
Epoch: 84  lr: 0.100
Epoch: [84][000/391]	Time 1.1084 Data 0.8332 Loss 0.4239 (0.4239)	Acc@1 89.062 (89.062)	Acc@5 98.438 (98.438)
Epoch: [84][100/391]	Time 0.1902 Data 0.0020 Loss 0.2917 (0.2697)	Acc@1 85.938 (91.615)	Acc@5 100.000 (99.629)
Epoch: [84][200/391]	Time 0.1893 Data 0.0022 Loss 0.3899 (0.2742)	Acc@1 88.281 (91.441)	Acc@5 99.219 (99.611)
Epoch: [84][300/391]	Time 0.1907 Data 0.0033 Loss 0.2997 (0.2883)	Acc@1 86.719 (91.014)	Acc@5 100.000 (99.564)
Testing the models......
Loss: 1.5769, Prec@1: 65.74, Prec@5: 88.06
Epoch time: 82s
Epoch: 85  lr: 0.100
Epoch: [85][000/391]	Time 1.0199 Data 0.8272 Loss 0.2939 (0.2939)	Acc@1 90.625 (90.625)	Acc@5 100.000 (100.000)
Epoch: [85][100/391]	Time 0.1901 Data 0.0025 Loss 0.2301 (0.2657)	Acc@1 91.406 (91.963)	Acc@5 100.000 (99.536)
Epoch: [85][200/391]	Time 0.1906 Data 0.0021 Loss 0.3837 (0.2848)	Acc@1 89.844 (91.200)	Acc@5 97.656 (99.495)
Epoch: [85][300/391]	Time 0.1934 Data 0.0021 Loss 0.3345 (0.3076)	Acc@1 89.844 (90.472)	Acc@5 99.219 (99.421)
Testing the models......
Loss: 1.6573, Prec@1: 64.62, Prec@5: 87.92
Epoch time: 82s
Epoch: 86  lr: 0.100
Epoch: [86][000/391]	Time 1.0931 Data 0.9337 Loss 0.2088 (0.2088)	Acc@1 94.531 (94.531)	Acc@5 100.000 (100.000)
Epoch: [86][100/391]	Time 0.1907 Data 0.0020 Loss 0.2211 (0.2924)	Acc@1 91.406 (90.671)	Acc@5 100.000 (99.513)
Epoch: [86][200/391]	Time 0.1904 Data 0.0021 Loss 0.3922 (0.2831)	Acc@1 89.844 (91.123)	Acc@5 99.219 (99.526)
Epoch: [86][300/391]	Time 0.1915 Data 0.0020 Loss 0.3489 (0.2974)	Acc@1 89.062 (90.695)	Acc@5 98.438 (99.476)
Testing the models......
Loss: 1.6969, Prec@1: 62.76, Prec@5: 87.52
Epoch time: 82s
Epoch: 87  lr: 0.100
Epoch: [87][000/391]	Time 1.1008 Data 0.9206 Loss 0.2575 (0.2575)	Acc@1 89.062 (89.062)	Acc@5 100.000 (100.000)
Epoch: [87][100/391]	Time 0.1897 Data 0.0020 Loss 0.2616 (0.2632)	Acc@1 91.406 (91.638)	Acc@5 99.219 (99.575)
Epoch: [87][200/391]	Time 0.1893 Data 0.0021 Loss 0.2988 (0.2796)	Acc@1 90.625 (91.224)	Acc@5 100.000 (99.569)
Epoch: [87][300/391]	Time 0.1899 Data 0.0021 Loss 0.2793 (0.2893)	Acc@1 92.188 (90.952)	Acc@5 100.000 (99.567)
Testing the models......
Loss: 1.6349, Prec@1: 65.30, Prec@5: 87.27
Epoch time: 82s
Epoch: 88  lr: 0.100
Epoch: [88][000/391]	Time 1.0383 Data 0.7817 Loss 0.2327 (0.2327)	Acc@1 92.188 (92.188)	Acc@5 99.219 (99.219)
Epoch: [88][100/391]	Time 0.1901 Data 0.0021 Loss 0.4170 (0.2588)	Acc@1 89.062 (91.824)	Acc@5 99.219 (99.590)
Epoch: [88][200/391]	Time 0.1954 Data 0.0021 Loss 0.3847 (0.2767)	Acc@1 87.500 (91.430)	Acc@5 99.219 (99.534)
Epoch: [88][300/391]	Time 0.1919 Data 0.0029 Loss 0.4014 (0.3017)	Acc@1 89.062 (90.630)	Acc@5 98.438 (99.426)
Testing the models......
Loss: 1.6073, Prec@1: 65.21, Prec@5: 88.27
Epoch time: 82s
Epoch: 89  lr: 0.100
Epoch: [89][000/391]	Time 1.0747 Data 0.8620 Loss 0.2190 (0.2190)	Acc@1 91.406 (91.406)	Acc@5 100.000 (100.000)
Epoch: [89][100/391]	Time 0.1898 Data 0.0028 Loss 0.5712 (0.2539)	Acc@1 86.719 (92.087)	Acc@5 98.438 (99.482)
Epoch: [89][200/391]	Time 0.1973 Data 0.0022 Loss 0.2303 (0.2766)	Acc@1 94.531 (91.325)	Acc@5 100.000 (99.530)
Epoch: [89][300/391]	Time 0.1890 Data 0.0021 Loss 0.3592 (0.2869)	Acc@1 89.062 (91.066)	Acc@5 97.656 (99.486)
Testing the models......
Loss: 1.5607, Prec@1: 65.53, Prec@5: 87.78
Epoch time: 82s
Epoch: 90  lr: 0.100
Epoch: [90][000/391]	Time 1.2106 Data 1.0327 Loss 0.2577 (0.2577)	Acc@1 92.969 (92.969)	Acc@5 99.219 (99.219)
Epoch: [90][100/391]	Time 0.1894 Data 0.0021 Loss 0.3090 (0.2629)	Acc@1 90.625 (91.785)	Acc@5 99.219 (99.544)
Epoch: [90][200/391]	Time 0.1896 Data 0.0026 Loss 0.2752 (0.2778)	Acc@1 89.062 (91.262)	Acc@5 98.438 (99.487)
Epoch: [90][300/391]	Time 0.1902 Data 0.0022 Loss 0.2572 (0.2906)	Acc@1 93.750 (90.911)	Acc@5 99.219 (99.421)
Testing the models......
Loss: 1.5148, Prec@1: 66.14, Prec@5: 88.18
Epoch time: 82s
Epoch: 91  lr: 0.100
Epoch: [91][000/391]	Time 1.0518 Data 0.8505 Loss 0.3165 (0.3165)	Acc@1 89.844 (89.844)	Acc@5 98.438 (98.438)
Epoch: [91][100/391]	Time 0.1925 Data 0.0020 Loss 0.3786 (0.2691)	Acc@1 88.281 (91.824)	Acc@5 99.219 (99.621)
Epoch: [91][200/391]	Time 0.1893 Data 0.0022 Loss 0.3230 (0.2772)	Acc@1 89.062 (91.437)	Acc@5 99.219 (99.541)
Epoch: [91][300/391]	Time 0.1885 Data 0.0030 Loss 0.3234 (0.2893)	Acc@1 90.625 (91.001)	Acc@5 100.000 (99.496)
Testing the models......
Loss: 1.6494, Prec@1: 64.36, Prec@5: 87.97
Epoch time: 82s
Epoch: 92  lr: 0.100
Epoch: [92][000/391]	Time 1.0671 Data 0.7943 Loss 0.3078 (0.3078)	Acc@1 91.406 (91.406)	Acc@5 99.219 (99.219)
Epoch: [92][100/391]	Time 0.1908 Data 0.0021 Loss 0.2324 (0.2559)	Acc@1 94.531 (92.041)	Acc@5 100.000 (99.536)
Epoch: [92][200/391]	Time 0.1899 Data 0.0022 Loss 0.2799 (0.2813)	Acc@1 92.969 (91.274)	Acc@5 100.000 (99.475)
Epoch: [92][300/391]	Time 0.1892 Data 0.0031 Loss 0.2494 (0.2890)	Acc@1 90.625 (90.968)	Acc@5 100.000 (99.478)
Testing the models......
Loss: 1.6221, Prec@1: 66.06, Prec@5: 87.73
Epoch time: 82s
Epoch: 93  lr: 0.100
Epoch: [93][000/391]	Time 1.2963 Data 1.1318 Loss 0.2334 (0.2334)	Acc@1 92.969 (92.969)	Acc@5 100.000 (100.000)
Epoch: [93][100/391]	Time 0.1899 Data 0.0029 Loss 0.2370 (0.2761)	Acc@1 89.062 (91.460)	Acc@5 100.000 (99.551)
Epoch: [93][200/391]	Time 0.1912 Data 0.0021 Loss 0.2610 (0.2774)	Acc@1 91.406 (91.550)	Acc@5 100.000 (99.541)
Epoch: [93][300/391]	Time 0.1897 Data 0.0022 Loss 0.3321 (0.2851)	Acc@1 91.406 (91.279)	Acc@5 100.000 (99.517)
Testing the models......
Loss: 1.6854, Prec@1: 64.37, Prec@5: 86.53
Epoch time: 82s
Epoch: 94  lr: 0.100
Epoch: [94][000/391]	Time 1.0666 Data 0.8576 Loss 0.2300 (0.2300)	Acc@1 92.969 (92.969)	Acc@5 100.000 (100.000)
Epoch: [94][100/391]	Time 0.1932 Data 0.0021 Loss 0.2411 (0.2623)	Acc@1 92.188 (92.010)	Acc@5 100.000 (99.644)
Epoch: [94][200/391]	Time 0.1902 Data 0.0030 Loss 0.2139 (0.2662)	Acc@1 92.188 (91.752)	Acc@5 100.000 (99.611)
Epoch: [94][300/391]	Time 0.1942 Data 0.0023 Loss 0.3093 (0.2779)	Acc@1 88.281 (91.409)	Acc@5 100.000 (99.520)
Testing the models......
Loss: 1.6276, Prec@1: 65.20, Prec@5: 87.01
Epoch time: 82s
Epoch: 95  lr: 0.100
Epoch: [95][000/391]	Time 1.0767 Data 0.8915 Loss 0.1990 (0.1990)	Acc@1 93.750 (93.750)	Acc@5 98.438 (98.438)
Epoch: [95][100/391]	Time 0.1895 Data 0.0020 Loss 0.3227 (0.2887)	Acc@1 88.281 (91.004)	Acc@5 99.219 (99.551)
Epoch: [95][200/391]	Time 0.1894 Data 0.0023 Loss 0.3762 (0.2957)	Acc@1 86.719 (90.621)	Acc@5 100.000 (99.553)
Epoch: [95][300/391]	Time 0.1892 Data 0.0020 Loss 0.3980 (0.3099)	Acc@1 84.375 (90.350)	Acc@5 99.219 (99.403)
Testing the models......
Loss: 1.6307, Prec@1: 65.39, Prec@5: 87.81
Epoch time: 82s
Epoch: 96  lr: 0.100
Epoch: [96][000/391]	Time 1.0551 Data 0.8397 Loss 0.3840 (0.3840)	Acc@1 87.500 (87.500)	Acc@5 99.219 (99.219)
Epoch: [96][100/391]	Time 0.1891 Data 0.0028 Loss 0.3715 (0.2915)	Acc@1 87.500 (90.989)	Acc@5 99.219 (99.435)
Epoch: [96][200/391]	Time 0.1892 Data 0.0021 Loss 0.2921 (0.2865)	Acc@1 91.406 (91.068)	Acc@5 98.438 (99.475)
Epoch: [96][300/391]	Time 0.1913 Data 0.0021 Loss 0.3367 (0.2960)	Acc@1 89.844 (90.773)	Acc@5 100.000 (99.419)
Testing the models......
Loss: 1.6159, Prec@1: 65.04, Prec@5: 88.17
Epoch time: 82s
Epoch: 97  lr: 0.100
Epoch: [97][000/391]	Time 0.9843 Data 0.7773 Loss 0.2944 (0.2944)	Acc@1 91.406 (91.406)	Acc@5 99.219 (99.219)
Epoch: [97][100/391]	Time 0.1902 Data 0.0028 Loss 0.2619 (0.2546)	Acc@1 94.531 (92.389)	Acc@5 99.219 (99.466)
Epoch: [97][200/391]	Time 0.1917 Data 0.0022 Loss 0.3700 (0.2707)	Acc@1 87.500 (91.772)	Acc@5 100.000 (99.429)
Epoch: [97][300/391]	Time 0.1906 Data 0.0021 Loss 0.4835 (0.2831)	Acc@1 80.469 (91.302)	Acc@5 99.219 (99.413)
Testing the models......
Loss: 1.6026, Prec@1: 65.10, Prec@5: 87.59
Epoch time: 82s
Epoch: 98  lr: 0.100
Epoch: [98][000/391]	Time 1.1700 Data 0.9556 Loss 0.2596 (0.2596)	Acc@1 89.844 (89.844)	Acc@5 100.000 (100.000)
Epoch: [98][100/391]	Time 0.1902 Data 0.0026 Loss 0.2900 (0.2797)	Acc@1 92.188 (91.197)	Acc@5 99.219 (99.606)
Epoch: [98][200/391]	Time 0.1901 Data 0.0023 Loss 0.3614 (0.2773)	Acc@1 88.281 (91.305)	Acc@5 99.219 (99.584)
Epoch: [98][300/391]	Time 0.1929 Data 0.0021 Loss 0.3140 (0.2829)	Acc@1 89.844 (91.144)	Acc@5 100.000 (99.559)
Testing the models......
Loss: 1.6338, Prec@1: 64.63, Prec@5: 88.00
Epoch time: 82s
Epoch: 99  lr: 0.100
Epoch: [99][000/391]	Time 0.9918 Data 0.7835 Loss 0.2760 (0.2760)	Acc@1 91.406 (91.406)	Acc@5 100.000 (100.000)
Epoch: [99][100/391]	Time 0.1909 Data 0.0020 Loss 0.3226 (0.2756)	Acc@1 89.844 (91.460)	Acc@5 99.219 (99.536)
Epoch: [99][200/391]	Time 0.1895 Data 0.0026 Loss 0.3993 (0.2819)	Acc@1 87.500 (91.294)	Acc@5 98.438 (99.549)
Epoch: [99][300/391]	Time 0.1919 Data 0.0021 Loss 0.2527 (0.2932)	Acc@1 92.969 (90.960)	Acc@5 100.000 (99.468)
Testing the models......
Loss: 1.6413, Prec@1: 65.80, Prec@5: 88.04
Epoch time: 82s
Epoch: 100  lr: 0.010
Epoch: [100][000/391]	Time 1.0838 Data 0.8845 Loss 0.1832 (0.1832)	Acc@1 93.750 (93.750)	Acc@5 100.000 (100.000)
Epoch: [100][100/391]	Time 0.1897 Data 0.0029 Loss 0.1486 (0.1575)	Acc@1 96.875 (95.521)	Acc@5 100.000 (99.830)
Epoch: [100][200/391]	Time 0.1898 Data 0.0026 Loss 0.0498 (0.1271)	Acc@1 98.438 (96.545)	Acc@5 100.000 (99.903)
Epoch: [100][300/391]	Time 0.1902 Data 0.0028 Loss 0.0749 (0.1118)	Acc@1 98.438 (97.049)	Acc@5 100.000 (99.927)
Testing the models......
Loss: 1.0835, Prec@1: 73.63, Prec@5: 92.11
Epoch time: 82s
Saving models
Epoch: 101  lr: 0.010
Epoch: [101][000/391]	Time 1.0158 Data 0.7610 Loss 0.0306 (0.0306)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [101][100/391]	Time 0.1899 Data 0.0021 Loss 0.0557 (0.0582)	Acc@1 99.219 (98.917)	Acc@5 100.000 (100.000)
Epoch: [101][200/391]	Time 0.1891 Data 0.0021 Loss 0.1289 (0.0564)	Acc@1 96.875 (99.001)	Acc@5 100.000 (99.996)
Epoch: [101][300/391]	Time 0.1943 Data 0.0020 Loss 0.0404 (0.0561)	Acc@1 100.000 (98.993)	Acc@5 100.000 (99.997)
Testing the models......
Loss: 1.0489, Prec@1: 74.24, Prec@5: 92.32
Epoch time: 82s
Saving models
Epoch: 102  lr: 0.010
Epoch: [102][000/391]	Time 1.0016 Data 0.8027 Loss 0.0400 (0.0400)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [102][100/391]	Time 0.1914 Data 0.0021 Loss 0.0373 (0.0458)	Acc@1 100.000 (99.397)	Acc@5 100.000 (100.000)
Epoch: [102][200/391]	Time 0.1901 Data 0.0027 Loss 0.0334 (0.0449)	Acc@1 100.000 (99.429)	Acc@5 100.000 (100.000)
Epoch: [102][300/391]	Time 0.1919 Data 0.0022 Loss 0.0307 (0.0438)	Acc@1 100.000 (99.432)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 1.0432, Prec@1: 74.35, Prec@5: 92.37
Epoch time: 82s
Saving models
Epoch: 103  lr: 0.010
Epoch: [103][000/391]	Time 1.0571 Data 0.7955 Loss 0.0300 (0.0300)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [103][100/391]	Time 0.1900 Data 0.0020 Loss 0.0481 (0.0407)	Acc@1 99.219 (99.435)	Acc@5 100.000 (100.000)
Epoch: [103][200/391]	Time 0.1884 Data 0.0021 Loss 0.0263 (0.0401)	Acc@1 100.000 (99.464)	Acc@5 100.000 (100.000)
Epoch: [103][300/391]	Time 0.1927 Data 0.0021 Loss 0.0310 (0.0391)	Acc@1 100.000 (99.507)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 1.0274, Prec@1: 74.41, Prec@5: 92.56
Epoch time: 81s
Saving models
Epoch: 104  lr: 0.010
Epoch: [104][000/391]	Time 0.9845 Data 0.7570 Loss 0.0184 (0.0184)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [104][100/391]	Time 0.1901 Data 0.0021 Loss 0.0477 (0.0347)	Acc@1 99.219 (99.706)	Acc@5 100.000 (100.000)
Epoch: [104][200/391]	Time 0.1895 Data 0.0020 Loss 0.0377 (0.0350)	Acc@1 99.219 (99.658)	Acc@5 100.000 (100.000)
Epoch: [104][300/391]	Time 0.1890 Data 0.0025 Loss 0.0470 (0.0351)	Acc@1 100.000 (99.668)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 1.0147, Prec@1: 74.85, Prec@5: 92.65
Epoch time: 81s
Saving models
Epoch: 105  lr: 0.010
Epoch: [105][000/391]	Time 1.0220 Data 0.8193 Loss 0.0250 (0.0250)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [105][100/391]	Time 0.1888 Data 0.0019 Loss 0.0204 (0.0310)	Acc@1 100.000 (99.783)	Acc@5 100.000 (100.000)
Epoch: [105][200/391]	Time 0.1888 Data 0.0023 Loss 0.0263 (0.0322)	Acc@1 100.000 (99.716)	Acc@5 100.000 (100.000)
Epoch: [105][300/391]	Time 0.1883 Data 0.0021 Loss 0.0255 (0.0324)	Acc@1 100.000 (99.722)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 1.0133, Prec@1: 75.01, Prec@5: 92.43
Epoch time: 81s
Saving models
Epoch: 106  lr: 0.010
Epoch: [106][000/391]	Time 1.2829 Data 1.0580 Loss 0.0267 (0.0267)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [106][100/391]	Time 0.1888 Data 0.0021 Loss 0.0370 (0.0305)	Acc@1 99.219 (99.791)	Acc@5 100.000 (100.000)
Epoch: [106][200/391]	Time 0.1883 Data 0.0022 Loss 0.0172 (0.0299)	Acc@1 100.000 (99.794)	Acc@5 100.000 (100.000)
Epoch: [106][300/391]	Time 0.1891 Data 0.0024 Loss 0.0189 (0.0302)	Acc@1 100.000 (99.766)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 1.0115, Prec@1: 75.04, Prec@5: 92.42
Epoch time: 82s
Saving models
Epoch: 107  lr: 0.010
Epoch: [107][000/391]	Time 1.1583 Data 0.9792 Loss 0.0382 (0.0382)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [107][100/391]	Time 0.1901 Data 0.0020 Loss 0.0356 (0.0286)	Acc@1 100.000 (99.768)	Acc@5 100.000 (100.000)
Epoch: [107][200/391]	Time 0.1891 Data 0.0021 Loss 0.0222 (0.0283)	Acc@1 100.000 (99.794)	Acc@5 100.000 (100.000)
Epoch: [107][300/391]	Time 0.1888 Data 0.0020 Loss 0.0335 (0.0285)	Acc@1 99.219 (99.798)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 1.0079, Prec@1: 75.06, Prec@5: 92.39
Epoch time: 82s
Saving models
Epoch: 108  lr: 0.010
Epoch: [108][000/391]	Time 1.0608 Data 0.8691 Loss 0.0235 (0.0235)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [108][100/391]	Time 0.1894 Data 0.0021 Loss 0.0220 (0.0270)	Acc@1 100.000 (99.884)	Acc@5 100.000 (100.000)
Epoch: [108][200/391]	Time 0.1893 Data 0.0023 Loss 0.0208 (0.0272)	Acc@1 100.000 (99.852)	Acc@5 100.000 (100.000)
Epoch: [108][300/391]	Time 0.1908 Data 0.0022 Loss 0.0231 (0.0272)	Acc@1 100.000 (99.852)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 1.0024, Prec@1: 75.13, Prec@5: 92.53
Epoch time: 81s
Saving models
Epoch: 109  lr: 0.010
Epoch: [109][000/391]	Time 1.0575 Data 0.7984 Loss 0.0161 (0.0161)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [109][100/391]	Time 0.1939 Data 0.0025 Loss 0.0289 (0.0260)	Acc@1 100.000 (99.814)	Acc@5 100.000 (100.000)
Epoch: [109][200/391]	Time 0.1891 Data 0.0052 Loss 0.0208 (0.0264)	Acc@1 100.000 (99.837)	Acc@5 100.000 (100.000)
Epoch: [109][300/391]	Time 0.1890 Data 0.0029 Loss 0.0478 (0.0266)	Acc@1 99.219 (99.842)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 1.0075, Prec@1: 75.07, Prec@5: 92.28
Epoch time: 82s
Epoch: 110  lr: 0.010
Epoch: [110][000/391]	Time 0.9723 Data 0.7877 Loss 0.0126 (0.0126)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [110][100/391]	Time 0.1893 Data 0.0022 Loss 0.0304 (0.0262)	Acc@1 100.000 (99.845)	Acc@5 100.000 (100.000)
Epoch: [110][200/391]	Time 0.1922 Data 0.0022 Loss 0.0422 (0.0261)	Acc@1 100.000 (99.872)	Acc@5 100.000 (100.000)
Epoch: [110][300/391]	Time 0.1898 Data 0.0021 Loss 0.0245 (0.0258)	Acc@1 100.000 (99.865)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9984, Prec@1: 75.32, Prec@5: 92.26
Epoch time: 82s
Saving models
Epoch: 111  lr: 0.010
Epoch: [111][000/391]	Time 1.0984 Data 0.8364 Loss 0.0210 (0.0210)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [111][100/391]	Time 0.1889 Data 0.0021 Loss 0.0287 (0.0246)	Acc@1 99.219 (99.868)	Acc@5 100.000 (100.000)
Epoch: [111][200/391]	Time 0.1963 Data 0.0022 Loss 0.0455 (0.0249)	Acc@1 99.219 (99.864)	Acc@5 100.000 (100.000)
Epoch: [111][300/391]	Time 0.1906 Data 0.0021 Loss 0.0293 (0.0249)	Acc@1 100.000 (99.868)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9934, Prec@1: 75.24, Prec@5: 92.35
Epoch time: 82s
Epoch: 112  lr: 0.010
Epoch: [112][000/391]	Time 1.0797 Data 0.8741 Loss 0.0202 (0.0202)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [112][100/391]	Time 0.1933 Data 0.0022 Loss 0.0220 (0.0248)	Acc@1 100.000 (99.861)	Acc@5 100.000 (100.000)
Epoch: [112][200/391]	Time 0.1906 Data 0.0026 Loss 0.0329 (0.0246)	Acc@1 99.219 (99.864)	Acc@5 100.000 (100.000)
Epoch: [112][300/391]	Time 0.1926 Data 0.0021 Loss 0.0134 (0.0244)	Acc@1 100.000 (99.873)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9926, Prec@1: 75.40, Prec@5: 92.37
Epoch time: 82s
Saving models
Epoch: 113  lr: 0.010
Epoch: [113][000/391]	Time 1.0553 Data 0.8740 Loss 0.0248 (0.0248)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [113][100/391]	Time 0.1930 Data 0.0020 Loss 0.0177 (0.0231)	Acc@1 100.000 (99.923)	Acc@5 100.000 (100.000)
Epoch: [113][200/391]	Time 0.1908 Data 0.0030 Loss 0.0162 (0.0234)	Acc@1 100.000 (99.911)	Acc@5 100.000 (100.000)
Epoch: [113][300/391]	Time 0.1954 Data 0.0020 Loss 0.0180 (0.0237)	Acc@1 100.000 (99.901)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9889, Prec@1: 75.46, Prec@5: 92.36
Epoch time: 82s
Saving models
Epoch: 114  lr: 0.010
Epoch: [114][000/391]	Time 1.0382 Data 0.8500 Loss 0.0296 (0.0296)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [114][100/391]	Time 0.1898 Data 0.0020 Loss 0.0248 (0.0226)	Acc@1 100.000 (99.954)	Acc@5 100.000 (100.000)
Epoch: [114][200/391]	Time 0.1912 Data 0.0021 Loss 0.0230 (0.0230)	Acc@1 100.000 (99.930)	Acc@5 100.000 (100.000)
Epoch: [114][300/391]	Time 0.1905 Data 0.0022 Loss 0.0158 (0.0231)	Acc@1 100.000 (99.917)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9883, Prec@1: 75.15, Prec@5: 92.42
Epoch time: 82s
Epoch: 115  lr: 0.010
Epoch: [115][000/391]	Time 1.0691 Data 0.8569 Loss 0.0108 (0.0108)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [115][100/391]	Time 0.1907 Data 0.0026 Loss 0.0162 (0.0236)	Acc@1 100.000 (99.868)	Acc@5 100.000 (100.000)
Epoch: [115][200/391]	Time 0.1925 Data 0.0021 Loss 0.0215 (0.0233)	Acc@1 100.000 (99.883)	Acc@5 100.000 (100.000)
Epoch: [115][300/391]	Time 0.1913 Data 0.0024 Loss 0.0175 (0.0229)	Acc@1 100.000 (99.896)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9900, Prec@1: 75.36, Prec@5: 92.33
Epoch time: 82s
Epoch: 116  lr: 0.010
Epoch: [116][000/391]	Time 1.0267 Data 0.7957 Loss 0.0334 (0.0334)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [116][100/391]	Time 0.1893 Data 0.0019 Loss 0.0194 (0.0222)	Acc@1 100.000 (99.938)	Acc@5 100.000 (100.000)
Epoch: [116][200/391]	Time 0.1897 Data 0.0021 Loss 0.0227 (0.0218)	Acc@1 100.000 (99.926)	Acc@5 100.000 (100.000)
Epoch: [116][300/391]	Time 0.1896 Data 0.0021 Loss 0.0211 (0.0219)	Acc@1 100.000 (99.922)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9924, Prec@1: 75.36, Prec@5: 92.37
Epoch time: 82s
Epoch: 117  lr: 0.010
Epoch: [117][000/391]	Time 1.0858 Data 0.8614 Loss 0.0251 (0.0251)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [117][100/391]	Time 0.1890 Data 0.0021 Loss 0.0310 (0.0211)	Acc@1 99.219 (99.907)	Acc@5 100.000 (100.000)
Epoch: [117][200/391]	Time 0.1891 Data 0.0021 Loss 0.0116 (0.0210)	Acc@1 100.000 (99.930)	Acc@5 100.000 (100.000)
Epoch: [117][300/391]	Time 0.1890 Data 0.0022 Loss 0.0224 (0.0211)	Acc@1 100.000 (99.943)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9869, Prec@1: 75.57, Prec@5: 92.34
Epoch time: 82s
Saving models
Epoch: 118  lr: 0.010
Epoch: [118][000/391]	Time 1.2274 Data 1.0257 Loss 0.0244 (0.0244)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [118][100/391]	Time 0.1889 Data 0.0022 Loss 0.0308 (0.0217)	Acc@1 100.000 (99.907)	Acc@5 100.000 (100.000)
Epoch: [118][200/391]	Time 0.1890 Data 0.0023 Loss 0.0294 (0.0216)	Acc@1 99.219 (99.911)	Acc@5 100.000 (100.000)
Epoch: [118][300/391]	Time 0.1909 Data 0.0019 Loss 0.0425 (0.0217)	Acc@1 99.219 (99.901)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9859, Prec@1: 75.48, Prec@5: 92.36
Epoch time: 82s
Epoch: 119  lr: 0.010
Epoch: [119][000/391]	Time 1.0979 Data 0.8697 Loss 0.0223 (0.0223)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [119][100/391]	Time 0.1882 Data 0.0021 Loss 0.0191 (0.0202)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [119][200/391]	Time 0.1902 Data 0.0020 Loss 0.0179 (0.0201)	Acc@1 100.000 (99.934)	Acc@5 100.000 (100.000)
Epoch: [119][300/391]	Time 0.1889 Data 0.0021 Loss 0.0236 (0.0207)	Acc@1 100.000 (99.927)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9913, Prec@1: 75.42, Prec@5: 92.29
Epoch time: 82s
Epoch: 120  lr: 0.010
Epoch: [120][000/391]	Time 1.0618 Data 0.8747 Loss 0.0268 (0.0268)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [120][100/391]	Time 0.1894 Data 0.0027 Loss 0.0265 (0.0197)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [120][200/391]	Time 0.1893 Data 0.0021 Loss 0.0185 (0.0199)	Acc@1 100.000 (99.949)	Acc@5 100.000 (100.000)
Epoch: [120][300/391]	Time 0.1913 Data 0.0020 Loss 0.0173 (0.0202)	Acc@1 100.000 (99.935)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9868, Prec@1: 75.60, Prec@5: 92.26
Epoch time: 81s
Saving models
Epoch: 121  lr: 0.010
Epoch: [121][000/391]	Time 1.0541 Data 0.7936 Loss 0.0110 (0.0110)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [121][100/391]	Time 0.1891 Data 0.0025 Loss 0.0199 (0.0209)	Acc@1 100.000 (99.915)	Acc@5 100.000 (100.000)
Epoch: [121][200/391]	Time 0.1908 Data 0.0026 Loss 0.0246 (0.0209)	Acc@1 100.000 (99.911)	Acc@5 100.000 (100.000)
Epoch: [121][300/391]	Time 0.1979 Data 0.0023 Loss 0.0162 (0.0205)	Acc@1 100.000 (99.922)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9838, Prec@1: 75.44, Prec@5: 92.36
Epoch time: 82s
Epoch: 122  lr: 0.010
Epoch: [122][000/391]	Time 1.2267 Data 1.0240 Loss 0.0140 (0.0140)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [122][100/391]	Time 0.1895 Data 0.0020 Loss 0.0188 (0.0208)	Acc@1 100.000 (99.923)	Acc@5 100.000 (100.000)
Epoch: [122][200/391]	Time 0.1884 Data 0.0020 Loss 0.0242 (0.0204)	Acc@1 100.000 (99.930)	Acc@5 100.000 (100.000)
Epoch: [122][300/391]	Time 0.1909 Data 0.0019 Loss 0.0112 (0.0203)	Acc@1 100.000 (99.940)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9821, Prec@1: 75.33, Prec@5: 92.50
Epoch time: 82s
Epoch: 123  lr: 0.010
Epoch: [123][000/391]	Time 1.0605 Data 0.7907 Loss 0.0188 (0.0188)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [123][100/391]	Time 0.1912 Data 0.0052 Loss 0.0168 (0.0187)	Acc@1 100.000 (99.954)	Acc@5 100.000 (100.000)
Epoch: [123][200/391]	Time 0.1887 Data 0.0021 Loss 0.0229 (0.0192)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Epoch: [123][300/391]	Time 0.1937 Data 0.0021 Loss 0.0183 (0.0195)	Acc@1 100.000 (99.938)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9866, Prec@1: 75.51, Prec@5: 92.19
Epoch time: 82s
Epoch: 124  lr: 0.010
Epoch: [124][000/391]	Time 1.0626 Data 0.8772 Loss 0.0188 (0.0188)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [124][100/391]	Time 0.1930 Data 0.0020 Loss 0.0202 (0.0193)	Acc@1 100.000 (99.938)	Acc@5 100.000 (100.000)
Epoch: [124][200/391]	Time 0.1900 Data 0.0023 Loss 0.0132 (0.0196)	Acc@1 100.000 (99.938)	Acc@5 100.000 (100.000)
Epoch: [124][300/391]	Time 0.1905 Data 0.0020 Loss 0.0161 (0.0196)	Acc@1 100.000 (99.938)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9884, Prec@1: 75.21, Prec@5: 92.16
Epoch time: 82s
Epoch: 125  lr: 0.010
Epoch: [125][000/391]	Time 1.0526 Data 0.9060 Loss 0.0186 (0.0186)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [125][100/391]	Time 0.1894 Data 0.0022 Loss 0.0149 (0.0188)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [125][200/391]	Time 0.1895 Data 0.0020 Loss 0.0244 (0.0193)	Acc@1 100.000 (99.942)	Acc@5 100.000 (100.000)
Epoch: [125][300/391]	Time 0.1905 Data 0.0020 Loss 0.0160 (0.0192)	Acc@1 100.000 (99.935)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9844, Prec@1: 75.60, Prec@5: 92.17
Epoch time: 82s
Epoch: 126  lr: 0.010
Epoch: [126][000/391]	Time 1.0826 Data 0.8879 Loss 0.0207 (0.0207)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [126][100/391]	Time 0.1903 Data 0.0021 Loss 0.0158 (0.0184)	Acc@1 100.000 (99.938)	Acc@5 100.000 (100.000)
Epoch: [126][200/391]	Time 0.1897 Data 0.0021 Loss 0.0165 (0.0189)	Acc@1 100.000 (99.934)	Acc@5 100.000 (100.000)
Epoch: [126][300/391]	Time 0.1898 Data 0.0026 Loss 0.0197 (0.0190)	Acc@1 100.000 (99.938)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9865, Prec@1: 75.53, Prec@5: 92.24
Epoch time: 82s
Epoch: 127  lr: 0.010
Epoch: [127][000/391]	Time 1.0662 Data 0.9061 Loss 0.0137 (0.0137)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [127][100/391]	Time 0.1923 Data 0.0021 Loss 0.0204 (0.0191)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [127][200/391]	Time 0.1908 Data 0.0021 Loss 0.0139 (0.0196)	Acc@1 100.000 (99.926)	Acc@5 100.000 (100.000)
Epoch: [127][300/391]	Time 0.1897 Data 0.0022 Loss 0.0137 (0.0193)	Acc@1 100.000 (99.943)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9810, Prec@1: 75.57, Prec@5: 92.30
Epoch time: 82s
Epoch: 128  lr: 0.010
Epoch: [128][000/391]	Time 1.1473 Data 0.9581 Loss 0.0155 (0.0155)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [128][100/391]	Time 0.1923 Data 0.0035 Loss 0.0146 (0.0185)	Acc@1 100.000 (99.938)	Acc@5 100.000 (100.000)
Epoch: [128][200/391]	Time 0.1915 Data 0.0034 Loss 0.0178 (0.0185)	Acc@1 100.000 (99.949)	Acc@5 100.000 (100.000)
Epoch: [128][300/391]	Time 0.1911 Data 0.0031 Loss 0.0210 (0.0186)	Acc@1 100.000 (99.940)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9801, Prec@1: 75.65, Prec@5: 92.22
Epoch time: 82s
Saving models
Epoch: 129  lr: 0.010
Epoch: [129][000/391]	Time 1.0216 Data 0.7924 Loss 0.0154 (0.0154)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [129][100/391]	Time 0.1895 Data 0.0021 Loss 0.0204 (0.0182)	Acc@1 100.000 (99.954)	Acc@5 100.000 (100.000)
Epoch: [129][200/391]	Time 0.1923 Data 0.0028 Loss 0.0248 (0.0181)	Acc@1 100.000 (99.969)	Acc@5 100.000 (100.000)
Epoch: [129][300/391]	Time 0.1941 Data 0.0020 Loss 0.0234 (0.0180)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9804, Prec@1: 75.54, Prec@5: 92.35
Epoch time: 82s
Epoch: 130  lr: 0.010
Epoch: [130][000/391]	Time 1.0646 Data 0.7998 Loss 0.0296 (0.0296)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [130][100/391]	Time 0.1898 Data 0.0022 Loss 0.0181 (0.0186)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [130][200/391]	Time 0.1897 Data 0.0024 Loss 0.0186 (0.0187)	Acc@1 100.000 (99.938)	Acc@5 100.000 (100.000)
Epoch: [130][300/391]	Time 0.1924 Data 0.0023 Loss 0.0179 (0.0188)	Acc@1 100.000 (99.938)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9801, Prec@1: 75.56, Prec@5: 92.26
Epoch time: 82s
Epoch: 131  lr: 0.010
Epoch: [131][000/391]	Time 1.0408 Data 0.8391 Loss 0.0149 (0.0149)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [131][100/391]	Time 0.1905 Data 0.0023 Loss 0.0178 (0.0179)	Acc@1 100.000 (99.938)	Acc@5 100.000 (100.000)
Epoch: [131][200/391]	Time 0.1898 Data 0.0020 Loss 0.0179 (0.0179)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Epoch: [131][300/391]	Time 0.1892 Data 0.0023 Loss 0.0155 (0.0183)	Acc@1 100.000 (99.951)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9745, Prec@1: 75.63, Prec@5: 92.41
Epoch time: 82s
Epoch: 132  lr: 0.010
Epoch: [132][000/391]	Time 1.1942 Data 0.9968 Loss 0.0195 (0.0195)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [132][100/391]	Time 0.1919 Data 0.0021 Loss 0.0172 (0.0175)	Acc@1 100.000 (99.977)	Acc@5 100.000 (100.000)
Epoch: [132][200/391]	Time 0.1887 Data 0.0025 Loss 0.0205 (0.0179)	Acc@1 100.000 (99.957)	Acc@5 100.000 (100.000)
Epoch: [132][300/391]	Time 0.1898 Data 0.0025 Loss 0.0147 (0.0181)	Acc@1 100.000 (99.943)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9777, Prec@1: 75.81, Prec@5: 92.46
Epoch time: 82s
Saving models
Epoch: 133  lr: 0.010
Epoch: [133][000/391]	Time 1.2057 Data 0.9591 Loss 0.0109 (0.0109)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [133][100/391]	Time 0.1890 Data 0.0021 Loss 0.0144 (0.0176)	Acc@1 100.000 (99.969)	Acc@5 100.000 (100.000)
Epoch: [133][200/391]	Time 0.1892 Data 0.0022 Loss 0.0267 (0.0181)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Epoch: [133][300/391]	Time 0.1892 Data 0.0022 Loss 0.0151 (0.0180)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9742, Prec@1: 75.61, Prec@5: 92.32
Epoch time: 82s
Epoch: 134  lr: 0.010
Epoch: [134][000/391]	Time 1.1003 Data 0.8823 Loss 0.0147 (0.0147)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [134][100/391]	Time 0.1892 Data 0.0022 Loss 0.0206 (0.0172)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [134][200/391]	Time 0.1897 Data 0.0021 Loss 0.0170 (0.0177)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Epoch: [134][300/391]	Time 0.1910 Data 0.0022 Loss 0.0164 (0.0178)	Acc@1 100.000 (99.948)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9759, Prec@1: 75.74, Prec@5: 92.32
Epoch time: 82s
Epoch: 135  lr: 0.010
Epoch: [135][000/391]	Time 1.0754 Data 0.8141 Loss 0.0133 (0.0133)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [135][100/391]	Time 0.1891 Data 0.0021 Loss 0.0144 (0.0171)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [135][200/391]	Time 0.1943 Data 0.0023 Loss 0.0151 (0.0171)	Acc@1 100.000 (99.957)	Acc@5 100.000 (100.000)
Epoch: [135][300/391]	Time 0.1929 Data 0.0024 Loss 0.0115 (0.0175)	Acc@1 100.000 (99.948)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9735, Prec@1: 75.68, Prec@5: 92.41
Epoch time: 82s
Epoch: 136  lr: 0.010
Epoch: [136][000/391]	Time 1.0558 Data 0.8443 Loss 0.0140 (0.0140)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [136][100/391]	Time 0.1889 Data 0.0020 Loss 0.0190 (0.0177)	Acc@1 100.000 (99.930)	Acc@5 100.000 (100.000)
Epoch: [136][200/391]	Time 0.1900 Data 0.0024 Loss 0.0118 (0.0173)	Acc@1 100.000 (99.942)	Acc@5 100.000 (100.000)
Epoch: [136][300/391]	Time 0.1893 Data 0.0021 Loss 0.0309 (0.0177)	Acc@1 99.219 (99.935)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9729, Prec@1: 75.63, Prec@5: 92.47
Epoch time: 82s
Epoch: 137  lr: 0.010
Epoch: [137][000/391]	Time 1.0260 Data 0.8223 Loss 0.0145 (0.0145)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [137][100/391]	Time 0.1916 Data 0.0024 Loss 0.0158 (0.0170)	Acc@1 100.000 (99.977)	Acc@5 100.000 (100.000)
Epoch: [137][200/391]	Time 0.1919 Data 0.0023 Loss 0.0118 (0.0174)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Epoch: [137][300/391]	Time 0.1887 Data 0.0021 Loss 0.0212 (0.0174)	Acc@1 100.000 (99.956)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9757, Prec@1: 75.81, Prec@5: 92.36
Epoch time: 82s
Epoch: 138  lr: 0.010
Epoch: [138][000/391]	Time 0.9866 Data 0.7269 Loss 0.0107 (0.0107)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [138][100/391]	Time 0.1922 Data 0.0027 Loss 0.0190 (0.0177)	Acc@1 100.000 (99.930)	Acc@5 100.000 (100.000)
Epoch: [138][200/391]	Time 0.1896 Data 0.0021 Loss 0.0189 (0.0175)	Acc@1 100.000 (99.942)	Acc@5 100.000 (100.000)
Epoch: [138][300/391]	Time 0.1896 Data 0.0020 Loss 0.0150 (0.0174)	Acc@1 100.000 (99.943)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9781, Prec@1: 75.77, Prec@5: 92.31
Epoch time: 82s
Epoch: 139  lr: 0.010
Epoch: [139][000/391]	Time 1.0544 Data 0.7807 Loss 0.0142 (0.0142)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [139][100/391]	Time 0.1906 Data 0.0022 Loss 0.0150 (0.0168)	Acc@1 100.000 (99.977)	Acc@5 100.000 (100.000)
Epoch: [139][200/391]	Time 0.1906 Data 0.0020 Loss 0.0175 (0.0169)	Acc@1 100.000 (99.969)	Acc@5 100.000 (100.000)
Epoch: [139][300/391]	Time 0.1893 Data 0.0022 Loss 0.0160 (0.0170)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9815, Prec@1: 75.70, Prec@5: 92.32
Epoch time: 82s
Epoch: 140  lr: 0.010
Epoch: [140][000/391]	Time 1.0301 Data 0.8319 Loss 0.0180 (0.0180)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [140][100/391]	Time 0.1923 Data 0.0028 Loss 0.0110 (0.0166)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [140][200/391]	Time 0.1897 Data 0.0021 Loss 0.0226 (0.0170)	Acc@1 100.000 (99.965)	Acc@5 100.000 (100.000)
Epoch: [140][300/391]	Time 0.1949 Data 0.0020 Loss 0.0204 (0.0169)	Acc@1 100.000 (99.951)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9755, Prec@1: 75.65, Prec@5: 92.31
Epoch time: 82s
Epoch: 141  lr: 0.010
Epoch: [141][000/391]	Time 1.0481 Data 0.8382 Loss 0.0127 (0.0127)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [141][100/391]	Time 0.1898 Data 0.0021 Loss 0.0147 (0.0166)	Acc@1 100.000 (99.969)	Acc@5 100.000 (100.000)
Epoch: [141][200/391]	Time 0.1927 Data 0.0022 Loss 0.0196 (0.0170)	Acc@1 100.000 (99.965)	Acc@5 100.000 (100.000)
Epoch: [141][300/391]	Time 0.1897 Data 0.0022 Loss 0.0279 (0.0171)	Acc@1 99.219 (99.953)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9737, Prec@1: 75.74, Prec@5: 92.39
Epoch time: 82s
Epoch: 142  lr: 0.010
Epoch: [142][000/391]	Time 1.1201 Data 0.9290 Loss 0.0156 (0.0156)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [142][100/391]	Time 0.1909 Data 0.0030 Loss 0.0223 (0.0166)	Acc@1 99.219 (99.915)	Acc@5 100.000 (100.000)
Epoch: [142][200/391]	Time 0.1923 Data 0.0022 Loss 0.0186 (0.0166)	Acc@1 100.000 (99.938)	Acc@5 100.000 (100.000)
Epoch: [142][300/391]	Time 0.1892 Data 0.0021 Loss 0.0188 (0.0169)	Acc@1 100.000 (99.935)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9797, Prec@1: 75.63, Prec@5: 92.19
Epoch time: 82s
Epoch: 143  lr: 0.010
Epoch: [143][000/391]	Time 1.1060 Data 0.9189 Loss 0.0103 (0.0103)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [143][100/391]	Time 0.1919 Data 0.0022 Loss 0.0139 (0.0157)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [143][200/391]	Time 0.1891 Data 0.0021 Loss 0.0178 (0.0164)	Acc@1 100.000 (99.965)	Acc@5 100.000 (100.000)
Epoch: [143][300/391]	Time 0.1887 Data 0.0022 Loss 0.0114 (0.0165)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9770, Prec@1: 75.65, Prec@5: 92.28
Epoch time: 82s
Epoch: 144  lr: 0.010
Epoch: [144][000/391]	Time 1.0601 Data 0.8555 Loss 0.0193 (0.0193)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [144][100/391]	Time 0.1892 Data 0.0028 Loss 0.0155 (0.0160)	Acc@1 100.000 (99.954)	Acc@5 100.000 (100.000)
Epoch: [144][200/391]	Time 0.1900 Data 0.0030 Loss 0.0173 (0.0168)	Acc@1 100.000 (99.949)	Acc@5 100.000 (100.000)
Epoch: [144][300/391]	Time 0.1905 Data 0.0021 Loss 0.0113 (0.0169)	Acc@1 100.000 (99.951)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9795, Prec@1: 75.77, Prec@5: 92.27
Epoch time: 82s
Epoch: 145  lr: 0.010
Epoch: [145][000/391]	Time 1.0664 Data 0.8676 Loss 0.0101 (0.0101)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [145][100/391]	Time 0.1928 Data 0.0032 Loss 0.0134 (0.0167)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [145][200/391]	Time 0.1907 Data 0.0029 Loss 0.0156 (0.0164)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [145][300/391]	Time 0.1883 Data 0.0020 Loss 0.0177 (0.0167)	Acc@1 100.000 (99.958)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9725, Prec@1: 75.65, Prec@5: 92.48
Epoch time: 82s
Epoch: 146  lr: 0.010
Epoch: [146][000/391]	Time 1.0804 Data 0.8660 Loss 0.0235 (0.0235)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [146][100/391]	Time 0.1906 Data 0.0023 Loss 0.0193 (0.0161)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [146][200/391]	Time 0.1890 Data 0.0079 Loss 0.0264 (0.0166)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Epoch: [146][300/391]	Time 0.1917 Data 0.0020 Loss 0.0172 (0.0164)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9730, Prec@1: 75.74, Prec@5: 92.29
Epoch time: 82s
Epoch: 147  lr: 0.010
Epoch: [147][000/391]	Time 1.1140 Data 0.9150 Loss 0.0253 (0.0253)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [147][100/391]	Time 0.1894 Data 0.0020 Loss 0.0148 (0.0167)	Acc@1 100.000 (99.938)	Acc@5 100.000 (100.000)
Epoch: [147][200/391]	Time 0.1891 Data 0.0022 Loss 0.0140 (0.0164)	Acc@1 100.000 (99.949)	Acc@5 100.000 (100.000)
Epoch: [147][300/391]	Time 0.1884 Data 0.0020 Loss 0.0202 (0.0166)	Acc@1 100.000 (99.945)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9761, Prec@1: 75.50, Prec@5: 92.43
Epoch time: 82s
Epoch: 148  lr: 0.010
Epoch: [148][000/391]	Time 0.9954 Data 0.7889 Loss 0.0126 (0.0126)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [148][100/391]	Time 0.1892 Data 0.0021 Loss 0.0180 (0.0167)	Acc@1 100.000 (99.938)	Acc@5 100.000 (100.000)
Epoch: [148][200/391]	Time 0.1898 Data 0.0021 Loss 0.0198 (0.0167)	Acc@1 100.000 (99.938)	Acc@5 100.000 (100.000)
Epoch: [148][300/391]	Time 0.1896 Data 0.0021 Loss 0.0179 (0.0164)	Acc@1 100.000 (99.948)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9729, Prec@1: 75.82, Prec@5: 92.29
Epoch time: 82s
Saving models
Epoch: 149  lr: 0.010
Epoch: [149][000/391]	Time 1.0199 Data 0.8275 Loss 0.0158 (0.0158)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [149][100/391]	Time 0.1963 Data 0.0024 Loss 0.0196 (0.0158)	Acc@1 100.000 (99.969)	Acc@5 100.000 (100.000)
Epoch: [149][200/391]	Time 0.1903 Data 0.0022 Loss 0.0157 (0.0161)	Acc@1 100.000 (99.965)	Acc@5 100.000 (100.000)
Epoch: [149][300/391]	Time 0.1882 Data 0.0020 Loss 0.0205 (0.0162)	Acc@1 100.000 (99.958)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9719, Prec@1: 75.67, Prec@5: 92.38
Epoch time: 82s
Epoch: 150  lr: 0.001
Epoch: [150][000/391]	Time 1.0311 Data 0.8364 Loss 0.0114 (0.0114)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [150][100/391]	Time 0.1891 Data 0.0020 Loss 0.0162 (0.0153)	Acc@1 100.000 (99.977)	Acc@5 100.000 (100.000)
Epoch: [150][200/391]	Time 0.1907 Data 0.0021 Loss 0.0091 (0.0155)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [150][300/391]	Time 0.1905 Data 0.0021 Loss 0.0195 (0.0158)	Acc@1 100.000 (99.958)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9781, Prec@1: 75.76, Prec@5: 92.30
Epoch time: 81s
Epoch: 151  lr: 0.001
Epoch: [151][000/391]	Time 1.0593 Data 0.8696 Loss 0.0193 (0.0193)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [151][100/391]	Time 0.1903 Data 0.0021 Loss 0.0140 (0.0156)	Acc@1 100.000 (99.969)	Acc@5 100.000 (100.000)
Epoch: [151][200/391]	Time 0.1917 Data 0.0021 Loss 0.0167 (0.0160)	Acc@1 100.000 (99.973)	Acc@5 100.000 (100.000)
Epoch: [151][300/391]	Time 0.1899 Data 0.0021 Loss 0.0268 (0.0157)	Acc@1 99.219 (99.971)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9743, Prec@1: 75.67, Prec@5: 92.32
Epoch time: 82s
Epoch: 152  lr: 0.001
Epoch: [152][000/391]	Time 1.0776 Data 0.8222 Loss 0.0155 (0.0155)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [152][100/391]	Time 0.1918 Data 0.0021 Loss 0.0113 (0.0151)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [152][200/391]	Time 0.1935 Data 0.0021 Loss 0.0142 (0.0153)	Acc@1 100.000 (99.973)	Acc@5 100.000 (100.000)
Epoch: [152][300/391]	Time 0.1899 Data 0.0026 Loss 0.0171 (0.0155)	Acc@1 100.000 (99.974)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9742, Prec@1: 75.67, Prec@5: 92.32
Epoch time: 82s
Epoch: 153  lr: 0.001
Epoch: [153][000/391]	Time 1.0480 Data 0.8554 Loss 0.0161 (0.0161)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [153][100/391]	Time 0.1905 Data 0.0022 Loss 0.0151 (0.0156)	Acc@1 100.000 (99.969)	Acc@5 100.000 (100.000)
Epoch: [153][200/391]	Time 0.1898 Data 0.0020 Loss 0.0150 (0.0158)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Epoch: [153][300/391]	Time 0.1906 Data 0.0020 Loss 0.0208 (0.0157)	Acc@1 100.000 (99.964)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9773, Prec@1: 75.80, Prec@5: 92.30
Epoch time: 82s
Epoch: 154  lr: 0.001
Epoch: [154][000/391]	Time 1.0475 Data 0.8820 Loss 0.0205 (0.0205)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [154][100/391]	Time 0.1936 Data 0.0022 Loss 0.0196 (0.0158)	Acc@1 100.000 (99.977)	Acc@5 100.000 (100.000)
Epoch: [154][200/391]	Time 0.1900 Data 0.0021 Loss 0.0143 (0.0157)	Acc@1 100.000 (99.977)	Acc@5 100.000 (100.000)
Epoch: [154][300/391]	Time 0.1903 Data 0.0020 Loss 0.0102 (0.0159)	Acc@1 100.000 (99.969)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9742, Prec@1: 75.72, Prec@5: 92.35
Epoch time: 82s
Epoch: 155  lr: 0.001
Epoch: [155][000/391]	Time 1.0999 Data 0.9110 Loss 0.0176 (0.0176)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [155][100/391]	Time 0.1900 Data 0.0028 Loss 0.0170 (0.0160)	Acc@1 100.000 (99.938)	Acc@5 100.000 (100.000)
Epoch: [155][200/391]	Time 0.1905 Data 0.0021 Loss 0.0138 (0.0160)	Acc@1 100.000 (99.949)	Acc@5 100.000 (100.000)
Epoch: [155][300/391]	Time 0.1941 Data 0.0021 Loss 0.0175 (0.0161)	Acc@1 100.000 (99.945)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9730, Prec@1: 75.78, Prec@5: 92.37
Epoch time: 82s
Epoch: 156  lr: 0.001
Epoch: [156][000/391]	Time 1.1424 Data 0.9360 Loss 0.0203 (0.0203)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [156][100/391]	Time 0.1911 Data 0.0021 Loss 0.0120 (0.0160)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [156][200/391]	Time 0.1894 Data 0.0022 Loss 0.0150 (0.0157)	Acc@1 100.000 (99.957)	Acc@5 100.000 (100.000)
Epoch: [156][300/391]	Time 0.1916 Data 0.0021 Loss 0.0094 (0.0158)	Acc@1 100.000 (99.956)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9744, Prec@1: 75.80, Prec@5: 92.33
Epoch time: 82s
Epoch: 157  lr: 0.001
Epoch: [157][000/391]	Time 1.0124 Data 0.7940 Loss 0.0134 (0.0134)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [157][100/391]	Time 0.1891 Data 0.0022 Loss 0.0169 (0.0159)	Acc@1 100.000 (99.985)	Acc@5 100.000 (100.000)
Epoch: [157][200/391]	Time 0.1901 Data 0.0021 Loss 0.0138 (0.0158)	Acc@1 100.000 (99.981)	Acc@5 100.000 (100.000)
Epoch: [157][300/391]	Time 0.1889 Data 0.0027 Loss 0.0108 (0.0155)	Acc@1 100.000 (99.974)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9753, Prec@1: 75.81, Prec@5: 92.35
Epoch time: 82s
Epoch: 158  lr: 0.001
Epoch: [158][000/391]	Time 1.2099 Data 0.9847 Loss 0.0170 (0.0170)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [158][100/391]	Time 0.1915 Data 0.0021 Loss 0.0135 (0.0158)	Acc@1 100.000 (99.969)	Acc@5 100.000 (100.000)
Epoch: [158][200/391]	Time 0.1903 Data 0.0025 Loss 0.0157 (0.0155)	Acc@1 100.000 (99.969)	Acc@5 100.000 (100.000)
Epoch: [158][300/391]	Time 0.1888 Data 0.0022 Loss 0.0102 (0.0156)	Acc@1 100.000 (99.958)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9732, Prec@1: 75.74, Prec@5: 92.30
Epoch time: 82s
Epoch: 159  lr: 0.001
Epoch: [159][000/391]	Time 1.0316 Data 0.7596 Loss 0.0161 (0.0161)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [159][100/391]	Time 0.1903 Data 0.0027 Loss 0.0126 (0.0160)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [159][200/391]	Time 0.1894 Data 0.0019 Loss 0.0130 (0.0160)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Epoch: [159][300/391]	Time 0.1885 Data 0.0022 Loss 0.0208 (0.0158)	Acc@1 100.000 (99.956)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9753, Prec@1: 75.77, Prec@5: 92.38
Epoch time: 81s
Epoch: 160  lr: 0.001
Epoch: [160][000/391]	Time 1.0484 Data 0.8527 Loss 0.0140 (0.0140)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [160][100/391]	Time 0.1896 Data 0.0027 Loss 0.0152 (0.0159)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [160][200/391]	Time 0.1893 Data 0.0019 Loss 0.0178 (0.0156)	Acc@1 100.000 (99.957)	Acc@5 100.000 (100.000)
Epoch: [160][300/391]	Time 0.1880 Data 0.0019 Loss 0.0188 (0.0157)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9744, Prec@1: 75.74, Prec@5: 92.44
Epoch time: 81s
Epoch: 161  lr: 0.001
Epoch: [161][000/391]	Time 1.2337 Data 1.0703 Loss 0.0131 (0.0131)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [161][100/391]	Time 0.1888 Data 0.0026 Loss 0.0156 (0.0158)	Acc@1 100.000 (99.938)	Acc@5 100.000 (100.000)
Epoch: [161][200/391]	Time 0.1915 Data 0.0021 Loss 0.0124 (0.0161)	Acc@1 100.000 (99.934)	Acc@5 100.000 (100.000)
Epoch: [161][300/391]	Time 0.1887 Data 0.0031 Loss 0.0184 (0.0159)	Acc@1 100.000 (99.948)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9740, Prec@1: 75.72, Prec@5: 92.38
Epoch time: 82s
Epoch: 162  lr: 0.001
Epoch: [162][000/391]	Time 1.0445 Data 0.7763 Loss 0.0133 (0.0133)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [162][100/391]	Time 0.1896 Data 0.0020 Loss 0.0099 (0.0164)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [162][200/391]	Time 0.1886 Data 0.0021 Loss 0.0117 (0.0162)	Acc@1 100.000 (99.942)	Acc@5 100.000 (100.000)
Epoch: [162][300/391]	Time 0.1889 Data 0.0026 Loss 0.0166 (0.0159)	Acc@1 100.000 (99.956)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9715, Prec@1: 75.79, Prec@5: 92.29
Epoch time: 81s
Epoch: 163  lr: 0.001
Epoch: [163][000/391]	Time 1.0067 Data 0.8608 Loss 0.0153 (0.0153)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [163][100/391]	Time 0.1900 Data 0.0020 Loss 0.0144 (0.0157)	Acc@1 100.000 (99.977)	Acc@5 100.000 (100.000)
Epoch: [163][200/391]	Time 0.1910 Data 0.0023 Loss 0.0154 (0.0154)	Acc@1 100.000 (99.981)	Acc@5 100.000 (100.000)
Epoch: [163][300/391]	Time 0.1889 Data 0.0019 Loss 0.0150 (0.0156)	Acc@1 100.000 (99.977)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9736, Prec@1: 75.73, Prec@5: 92.32
Epoch time: 81s
Epoch: 164  lr: 0.001
Epoch: [164][000/391]	Time 1.1631 Data 1.0075 Loss 0.0212 (0.0212)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [164][100/391]	Time 0.1893 Data 0.0021 Loss 0.0222 (0.0159)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [164][200/391]	Time 0.1895 Data 0.0022 Loss 0.0138 (0.0156)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Epoch: [164][300/391]	Time 0.1911 Data 0.0019 Loss 0.0117 (0.0157)	Acc@1 100.000 (99.956)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9747, Prec@1: 75.75, Prec@5: 92.41
Epoch time: 82s
Epoch: 165  lr: 0.001
Epoch: [165][000/391]	Time 1.0170 Data 0.8309 Loss 0.0153 (0.0153)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [165][100/391]	Time 0.1913 Data 0.0023 Loss 0.0098 (0.0154)	Acc@1 100.000 (99.954)	Acc@5 100.000 (100.000)
Epoch: [165][200/391]	Time 0.1902 Data 0.0032 Loss 0.0146 (0.0156)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [165][300/391]	Time 0.1912 Data 0.0033 Loss 0.0141 (0.0157)	Acc@1 100.000 (99.966)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9727, Prec@1: 75.84, Prec@5: 92.33
Epoch time: 82s
Saving models
Epoch: 166  lr: 0.001
Epoch: [166][000/391]	Time 1.0787 Data 0.8979 Loss 0.0155 (0.0155)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [166][100/391]	Time 0.1903 Data 0.0020 Loss 0.0164 (0.0157)	Acc@1 100.000 (99.977)	Acc@5 100.000 (100.000)
Epoch: [166][200/391]	Time 0.1906 Data 0.0021 Loss 0.0179 (0.0157)	Acc@1 100.000 (99.973)	Acc@5 100.000 (100.000)
Epoch: [166][300/391]	Time 0.1897 Data 0.0020 Loss 0.0162 (0.0156)	Acc@1 100.000 (99.979)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9756, Prec@1: 75.75, Prec@5: 92.38
Epoch time: 82s
Epoch: 167  lr: 0.001
Epoch: [167][000/391]	Time 1.0830 Data 0.8830 Loss 0.0103 (0.0103)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [167][100/391]	Time 0.1896 Data 0.0022 Loss 0.0136 (0.0160)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [167][200/391]	Time 0.1894 Data 0.0021 Loss 0.0135 (0.0156)	Acc@1 100.000 (99.981)	Acc@5 100.000 (100.000)
Epoch: [167][300/391]	Time 0.1909 Data 0.0021 Loss 0.0097 (0.0155)	Acc@1 100.000 (99.974)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9762, Prec@1: 75.74, Prec@5: 92.25
Epoch time: 82s
Epoch: 168  lr: 0.001
Epoch: [168][000/391]	Time 1.0104 Data 0.7936 Loss 0.0160 (0.0160)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [168][100/391]	Time 0.1901 Data 0.0021 Loss 0.0195 (0.0160)	Acc@1 100.000 (99.969)	Acc@5 100.000 (100.000)
Epoch: [168][200/391]	Time 0.1898 Data 0.0023 Loss 0.0136 (0.0153)	Acc@1 100.000 (99.981)	Acc@5 100.000 (100.000)
Epoch: [168][300/391]	Time 0.1899 Data 0.0026 Loss 0.0142 (0.0154)	Acc@1 100.000 (99.979)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9735, Prec@1: 75.79, Prec@5: 92.37
Epoch time: 82s
Epoch: 169  lr: 0.001
Epoch: [169][000/391]	Time 1.1477 Data 0.9631 Loss 0.0144 (0.0144)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [169][100/391]	Time 0.1936 Data 0.0023 Loss 0.0122 (0.0155)	Acc@1 100.000 (99.969)	Acc@5 100.000 (100.000)
Epoch: [169][200/391]	Time 0.1901 Data 0.0021 Loss 0.0146 (0.0157)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [169][300/391]	Time 0.1897 Data 0.0020 Loss 0.0127 (0.0156)	Acc@1 100.000 (99.958)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9764, Prec@1: 75.60, Prec@5: 92.38
Epoch time: 82s
Epoch: 170  lr: 0.001
Epoch: [170][000/391]	Time 1.1435 Data 0.8803 Loss 0.0137 (0.0137)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [170][100/391]	Time 0.1912 Data 0.0021 Loss 0.0155 (0.0161)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [170][200/391]	Time 0.1894 Data 0.0022 Loss 0.0204 (0.0156)	Acc@1 100.000 (99.965)	Acc@5 100.000 (100.000)
Epoch: [170][300/391]	Time 0.1910 Data 0.0022 Loss 0.0127 (0.0155)	Acc@1 100.000 (99.969)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9720, Prec@1: 75.79, Prec@5: 92.39
Epoch time: 82s
Epoch: 171  lr: 0.001
Epoch: [171][000/391]	Time 1.0824 Data 0.8932 Loss 0.0113 (0.0113)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [171][100/391]	Time 0.1916 Data 0.0023 Loss 0.0184 (0.0157)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [171][200/391]	Time 0.1910 Data 0.0022 Loss 0.0180 (0.0156)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Epoch: [171][300/391]	Time 0.1922 Data 0.0023 Loss 0.0171 (0.0155)	Acc@1 100.000 (99.964)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9710, Prec@1: 75.72, Prec@5: 92.35
Epoch time: 82s
Epoch: 172  lr: 0.001
Epoch: [172][000/391]	Time 1.2537 Data 0.9934 Loss 0.0168 (0.0168)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [172][100/391]	Time 0.1885 Data 0.0021 Loss 0.0133 (0.0160)	Acc@1 100.000 (99.954)	Acc@5 100.000 (100.000)
Epoch: [172][200/391]	Time 0.1895 Data 0.0022 Loss 0.0201 (0.0159)	Acc@1 99.219 (99.949)	Acc@5 100.000 (100.000)
Epoch: [172][300/391]	Time 0.1890 Data 0.0021 Loss 0.0142 (0.0157)	Acc@1 100.000 (99.956)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9733, Prec@1: 75.75, Prec@5: 92.40
Epoch time: 82s
Epoch: 173  lr: 0.001
Epoch: [173][000/391]	Time 1.0383 Data 0.8559 Loss 0.0154 (0.0154)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [173][100/391]	Time 0.1942 Data 0.0021 Loss 0.0165 (0.0154)	Acc@1 100.000 (99.985)	Acc@5 100.000 (100.000)
Epoch: [173][200/391]	Time 0.1886 Data 0.0020 Loss 0.0154 (0.0154)	Acc@1 100.000 (99.969)	Acc@5 100.000 (100.000)
Epoch: [173][300/391]	Time 0.1930 Data 0.0024 Loss 0.0152 (0.0155)	Acc@1 100.000 (99.971)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9736, Prec@1: 75.77, Prec@5: 92.30
Epoch time: 82s
Epoch: 174  lr: 0.001
Epoch: [174][000/391]	Time 1.0679 Data 0.8520 Loss 0.0172 (0.0172)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [174][100/391]	Time 0.1903 Data 0.0021 Loss 0.0187 (0.0150)	Acc@1 100.000 (99.992)	Acc@5 100.000 (100.000)
Epoch: [174][200/391]	Time 0.1920 Data 0.0022 Loss 0.0109 (0.0150)	Acc@1 100.000 (99.981)	Acc@5 100.000 (100.000)
Epoch: [174][300/391]	Time 0.1946 Data 0.0021 Loss 0.0180 (0.0154)	Acc@1 100.000 (99.971)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9701, Prec@1: 75.70, Prec@5: 92.42
Epoch time: 81s
Epoch: 175  lr: 0.001
Epoch: [175][000/391]	Time 1.2186 Data 0.9583 Loss 0.0126 (0.0126)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [175][100/391]	Time 0.1898 Data 0.0021 Loss 0.0161 (0.0155)	Acc@1 100.000 (99.977)	Acc@5 100.000 (100.000)
Epoch: [175][200/391]	Time 0.1897 Data 0.0021 Loss 0.0126 (0.0153)	Acc@1 100.000 (99.981)	Acc@5 100.000 (100.000)
Epoch: [175][300/391]	Time 0.1896 Data 0.0021 Loss 0.0188 (0.0157)	Acc@1 100.000 (99.958)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9693, Prec@1: 75.83, Prec@5: 92.45
Epoch time: 82s
Epoch: 176  lr: 0.001
Epoch: [176][000/391]	Time 1.1009 Data 0.8928 Loss 0.0123 (0.0123)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [176][100/391]	Time 0.1888 Data 0.0031 Loss 0.0172 (0.0158)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [176][200/391]	Time 0.1897 Data 0.0021 Loss 0.0172 (0.0155)	Acc@1 100.000 (99.969)	Acc@5 100.000 (100.000)
Epoch: [176][300/391]	Time 0.1918 Data 0.0022 Loss 0.0142 (0.0153)	Acc@1 100.000 (99.977)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9741, Prec@1: 75.70, Prec@5: 92.36
Epoch time: 82s
Epoch: 177  lr: 0.001
Epoch: [177][000/391]	Time 1.0094 Data 0.8186 Loss 0.0106 (0.0106)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [177][100/391]	Time 0.1898 Data 0.0024 Loss 0.0128 (0.0159)	Acc@1 100.000 (99.946)	Acc@5 100.000 (100.000)
Epoch: [177][200/391]	Time 0.1898 Data 0.0022 Loss 0.0157 (0.0156)	Acc@1 100.000 (99.957)	Acc@5 100.000 (100.000)
Epoch: [177][300/391]	Time 0.1918 Data 0.0020 Loss 0.0169 (0.0155)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9758, Prec@1: 75.68, Prec@5: 92.33
Epoch time: 82s
Epoch: 178  lr: 0.001
Epoch: [178][000/391]	Time 1.0204 Data 0.7980 Loss 0.0197 (0.0197)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [178][100/391]	Time 0.1903 Data 0.0021 Loss 0.0124 (0.0152)	Acc@1 100.000 (99.969)	Acc@5 100.000 (100.000)
Epoch: [178][200/391]	Time 0.1937 Data 0.0021 Loss 0.0167 (0.0153)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [178][300/391]	Time 0.1900 Data 0.0021 Loss 0.0118 (0.0154)	Acc@1 100.000 (99.964)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9744, Prec@1: 75.72, Prec@5: 92.37
Epoch time: 82s
Epoch: 179  lr: 0.001
Epoch: [179][000/391]	Time 1.1261 Data 0.9213 Loss 0.0164 (0.0164)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [179][100/391]	Time 0.1918 Data 0.0020 Loss 0.0196 (0.0158)	Acc@1 100.000 (99.954)	Acc@5 100.000 (100.000)
Epoch: [179][200/391]	Time 0.1951 Data 0.0021 Loss 0.0163 (0.0157)	Acc@1 100.000 (99.957)	Acc@5 100.000 (100.000)
Epoch: [179][300/391]	Time 0.1926 Data 0.0023 Loss 0.0123 (0.0157)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9755, Prec@1: 75.66, Prec@5: 92.35
Epoch time: 82s
Epoch: 180  lr: 0.001
Epoch: [180][000/391]	Time 1.1577 Data 0.9745 Loss 0.0150 (0.0150)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [180][100/391]	Time 0.1908 Data 0.0021 Loss 0.0181 (0.0151)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [180][200/391]	Time 0.1905 Data 0.0021 Loss 0.0140 (0.0152)	Acc@1 100.000 (99.988)	Acc@5 100.000 (100.000)
Epoch: [180][300/391]	Time 0.1899 Data 0.0020 Loss 0.0127 (0.0154)	Acc@1 100.000 (99.982)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9734, Prec@1: 75.62, Prec@5: 92.30
Epoch time: 82s
Epoch: 181  lr: 0.001
Epoch: [181][000/391]	Time 1.1087 Data 0.8998 Loss 0.0176 (0.0176)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [181][100/391]	Time 0.1911 Data 0.0054 Loss 0.0101 (0.0149)	Acc@1 100.000 (99.977)	Acc@5 100.000 (100.000)
Epoch: [181][200/391]	Time 0.1896 Data 0.0021 Loss 0.0119 (0.0154)	Acc@1 100.000 (99.965)	Acc@5 100.000 (100.000)
Epoch: [181][300/391]	Time 0.1890 Data 0.0020 Loss 0.0134 (0.0156)	Acc@1 100.000 (99.951)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9733, Prec@1: 75.69, Prec@5: 92.40
Epoch time: 82s
Epoch: 182  lr: 0.001
Epoch: [182][000/391]	Time 1.0560 Data 0.8656 Loss 0.0174 (0.0174)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [182][100/391]	Time 0.1899 Data 0.0021 Loss 0.0137 (0.0149)	Acc@1 100.000 (99.985)	Acc@5 100.000 (100.000)
Epoch: [182][200/391]	Time 0.1914 Data 0.0022 Loss 0.0125 (0.0151)	Acc@1 100.000 (99.969)	Acc@5 100.000 (100.000)
Epoch: [182][300/391]	Time 0.1928 Data 0.0022 Loss 0.0131 (0.0152)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9727, Prec@1: 75.76, Prec@5: 92.28
Epoch time: 82s
Epoch: 183  lr: 0.001
Epoch: [183][000/391]	Time 1.0115 Data 0.7833 Loss 0.0118 (0.0118)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [183][100/391]	Time 0.1938 Data 0.0020 Loss 0.0159 (0.0154)	Acc@1 100.000 (99.985)	Acc@5 100.000 (100.000)
Epoch: [183][200/391]	Time 0.1946 Data 0.0021 Loss 0.0162 (0.0155)	Acc@1 100.000 (99.981)	Acc@5 100.000 (100.000)
Epoch: [183][300/391]	Time 0.1921 Data 0.0023 Loss 0.0159 (0.0155)	Acc@1 100.000 (99.979)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9739, Prec@1: 75.60, Prec@5: 92.34
Epoch time: 82s
Epoch: 184  lr: 0.001
Epoch: [184][000/391]	Time 1.0816 Data 0.8829 Loss 0.0229 (0.0229)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [184][100/391]	Time 0.1900 Data 0.0022 Loss 0.0153 (0.0153)	Acc@1 100.000 (99.969)	Acc@5 100.000 (100.000)
Epoch: [184][200/391]	Time 0.1919 Data 0.0024 Loss 0.0237 (0.0154)	Acc@1 100.000 (99.957)	Acc@5 100.000 (100.000)
Epoch: [184][300/391]	Time 0.1912 Data 0.0022 Loss 0.0132 (0.0153)	Acc@1 100.000 (99.958)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9730, Prec@1: 75.60, Prec@5: 92.41
Epoch time: 82s
Epoch: 185  lr: 0.001
Epoch: [185][000/391]	Time 1.0735 Data 0.8720 Loss 0.0296 (0.0296)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [185][100/391]	Time 0.1922 Data 0.0028 Loss 0.0119 (0.0158)	Acc@1 100.000 (99.977)	Acc@5 100.000 (100.000)
Epoch: [185][200/391]	Time 0.1892 Data 0.0023 Loss 0.0112 (0.0162)	Acc@1 100.000 (99.957)	Acc@5 100.000 (100.000)
Epoch: [185][300/391]	Time 0.1891 Data 0.0020 Loss 0.0134 (0.0159)	Acc@1 100.000 (99.966)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9718, Prec@1: 75.87, Prec@5: 92.37
Epoch time: 81s
Saving models
Epoch: 186  lr: 0.001
Epoch: [186][000/391]	Time 1.0833 Data 0.8188 Loss 0.0175 (0.0175)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [186][100/391]	Time 0.1888 Data 0.0023 Loss 0.0187 (0.0161)	Acc@1 100.000 (99.923)	Acc@5 100.000 (100.000)
Epoch: [186][200/391]	Time 0.1905 Data 0.0024 Loss 0.0166 (0.0158)	Acc@1 100.000 (99.942)	Acc@5 100.000 (100.000)
Epoch: [186][300/391]	Time 0.1901 Data 0.0021 Loss 0.0140 (0.0155)	Acc@1 100.000 (99.958)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9720, Prec@1: 75.79, Prec@5: 92.44
Epoch time: 82s
Epoch: 187  lr: 0.001
Epoch: [187][000/391]	Time 1.0102 Data 0.8163 Loss 0.0180 (0.0180)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [187][100/391]	Time 0.1898 Data 0.0022 Loss 0.0095 (0.0151)	Acc@1 100.000 (99.954)	Acc@5 100.000 (100.000)
Epoch: [187][200/391]	Time 0.1889 Data 0.0021 Loss 0.0139 (0.0150)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Epoch: [187][300/391]	Time 0.1896 Data 0.0023 Loss 0.0154 (0.0152)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9708, Prec@1: 75.81, Prec@5: 92.33
Epoch time: 82s
Epoch: 188  lr: 0.001
Epoch: [188][000/391]	Time 1.1084 Data 0.9139 Loss 0.0121 (0.0121)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [188][100/391]	Time 0.1868 Data 0.0020 Loss 0.0141 (0.0155)	Acc@1 100.000 (99.985)	Acc@5 100.000 (100.000)
Epoch: [188][200/391]	Time 0.1898 Data 0.0025 Loss 0.0153 (0.0155)	Acc@1 100.000 (99.973)	Acc@5 100.000 (100.000)
Epoch: [188][300/391]	Time 0.1887 Data 0.0020 Loss 0.0140 (0.0155)	Acc@1 100.000 (99.958)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9723, Prec@1: 75.81, Prec@5: 92.32
Epoch time: 82s
Epoch: 189  lr: 0.001
Epoch: [189][000/391]	Time 1.0345 Data 0.7757 Loss 0.0166 (0.0166)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [189][100/391]	Time 0.1908 Data 0.0019 Loss 0.0159 (0.0153)	Acc@1 100.000 (99.977)	Acc@5 100.000 (100.000)
Epoch: [189][200/391]	Time 0.1899 Data 0.0022 Loss 0.0205 (0.0153)	Acc@1 100.000 (99.969)	Acc@5 100.000 (100.000)
Epoch: [189][300/391]	Time 0.1886 Data 0.0021 Loss 0.0128 (0.0154)	Acc@1 100.000 (99.964)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9710, Prec@1: 75.85, Prec@5: 92.41
Epoch time: 81s
Epoch: 190  lr: 0.001
Epoch: [190][000/391]	Time 1.0103 Data 0.8099 Loss 0.0183 (0.0183)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [190][100/391]	Time 0.1902 Data 0.0023 Loss 0.0259 (0.0159)	Acc@1 99.219 (99.938)	Acc@5 100.000 (100.000)
Epoch: [190][200/391]	Time 0.1900 Data 0.0022 Loss 0.0126 (0.0156)	Acc@1 100.000 (99.965)	Acc@5 100.000 (100.000)
Epoch: [190][300/391]	Time 0.1899 Data 0.0020 Loss 0.0153 (0.0157)	Acc@1 100.000 (99.958)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9729, Prec@1: 75.79, Prec@5: 92.30
Epoch time: 81s
Epoch: 191  lr: 0.001
Epoch: [191][000/391]	Time 1.0782 Data 0.8090 Loss 0.0155 (0.0155)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [191][100/391]	Time 0.1924 Data 0.0021 Loss 0.0113 (0.0152)	Acc@1 100.000 (99.977)	Acc@5 100.000 (100.000)
Epoch: [191][200/391]	Time 0.1893 Data 0.0021 Loss 0.0155 (0.0153)	Acc@1 100.000 (99.973)	Acc@5 100.000 (100.000)
Epoch: [191][300/391]	Time 0.1898 Data 0.0020 Loss 0.0244 (0.0155)	Acc@1 100.000 (99.966)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9709, Prec@1: 75.79, Prec@5: 92.42
Epoch time: 82s
Epoch: 192  lr: 0.001
Epoch: [192][000/391]	Time 0.9743 Data 0.7790 Loss 0.0200 (0.0200)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [192][100/391]	Time 0.1897 Data 0.0030 Loss 0.0154 (0.0151)	Acc@1 100.000 (99.985)	Acc@5 100.000 (100.000)
Epoch: [192][200/391]	Time 0.1899 Data 0.0022 Loss 0.0107 (0.0149)	Acc@1 100.000 (99.988)	Acc@5 100.000 (100.000)
Epoch: [192][300/391]	Time 0.1900 Data 0.0024 Loss 0.0230 (0.0150)	Acc@1 100.000 (99.982)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9730, Prec@1: 75.81, Prec@5: 92.37
Epoch time: 82s
Epoch: 193  lr: 0.001
Epoch: [193][000/391]	Time 1.0311 Data 0.8135 Loss 0.0147 (0.0147)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [193][100/391]	Time 0.1910 Data 0.0023 Loss 0.0179 (0.0154)	Acc@1 100.000 (99.969)	Acc@5 100.000 (100.000)
Epoch: [193][200/391]	Time 0.1910 Data 0.0031 Loss 0.0183 (0.0154)	Acc@1 100.000 (99.969)	Acc@5 100.000 (100.000)
Epoch: [193][300/391]	Time 0.1911 Data 0.0023 Loss 0.0180 (0.0153)	Acc@1 100.000 (99.979)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9710, Prec@1: 75.90, Prec@5: 92.48
Epoch time: 82s
Saving models
Epoch: 194  lr: 0.001
Epoch: [194][000/391]	Time 1.0887 Data 0.8116 Loss 0.0119 (0.0119)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [194][100/391]	Time 0.1892 Data 0.0020 Loss 0.0151 (0.0152)	Acc@1 100.000 (99.954)	Acc@5 100.000 (100.000)
Epoch: [194][200/391]	Time 0.1897 Data 0.0021 Loss 0.0170 (0.0152)	Acc@1 100.000 (99.953)	Acc@5 100.000 (100.000)
Epoch: [194][300/391]	Time 0.1893 Data 0.0021 Loss 0.0119 (0.0154)	Acc@1 100.000 (99.956)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9750, Prec@1: 75.76, Prec@5: 92.42
Epoch time: 82s
Epoch: 195  lr: 0.001
Epoch: [195][000/391]	Time 1.1047 Data 0.9140 Loss 0.0143 (0.0143)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [195][100/391]	Time 0.1904 Data 0.0021 Loss 0.0212 (0.0155)	Acc@1 100.000 (99.985)	Acc@5 100.000 (100.000)
Epoch: [195][200/391]	Time 0.1906 Data 0.0032 Loss 0.0126 (0.0152)	Acc@1 100.000 (99.973)	Acc@5 100.000 (100.000)
Epoch: [195][300/391]	Time 0.1905 Data 0.0047 Loss 0.0118 (0.0153)	Acc@1 100.000 (99.971)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9729, Prec@1: 75.94, Prec@5: 92.39
Epoch time: 82s
Saving models
Epoch: 196  lr: 0.001
Epoch: [196][000/391]	Time 1.0555 Data 0.8617 Loss 0.0138 (0.0138)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [196][100/391]	Time 0.1914 Data 0.0023 Loss 0.0164 (0.0151)	Acc@1 100.000 (99.985)	Acc@5 100.000 (100.000)
Epoch: [196][200/391]	Time 0.1933 Data 0.0024 Loss 0.0151 (0.0153)	Acc@1 100.000 (99.984)	Acc@5 100.000 (100.000)
Epoch: [196][300/391]	Time 0.1964 Data 0.0021 Loss 0.0110 (0.0154)	Acc@1 100.000 (99.977)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9748, Prec@1: 75.77, Prec@5: 92.48
Epoch time: 82s
Epoch: 197  lr: 0.001
Epoch: [197][000/391]	Time 1.0389 Data 0.8287 Loss 0.0216 (0.0216)	Acc@1 99.219 (99.219)	Acc@5 100.000 (100.000)
Epoch: [197][100/391]	Time 0.1906 Data 0.0024 Loss 0.0245 (0.0149)	Acc@1 99.219 (99.946)	Acc@5 100.000 (100.000)
Epoch: [197][200/391]	Time 0.1930 Data 0.0021 Loss 0.0154 (0.0155)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [197][300/391]	Time 0.1936 Data 0.0021 Loss 0.0151 (0.0155)	Acc@1 100.000 (99.966)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9711, Prec@1: 75.98, Prec@5: 92.45
Epoch time: 82s
Saving models
Epoch: 198  lr: 0.001
Epoch: [198][000/391]	Time 1.0259 Data 0.7977 Loss 0.0169 (0.0169)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [198][100/391]	Time 0.1919 Data 0.0020 Loss 0.0154 (0.0153)	Acc@1 100.000 (99.977)	Acc@5 100.000 (100.000)
Epoch: [198][200/391]	Time 0.1897 Data 0.0022 Loss 0.0387 (0.0153)	Acc@1 99.219 (99.961)	Acc@5 100.000 (100.000)
Epoch: [198][300/391]	Time 0.1889 Data 0.0020 Loss 0.0167 (0.0153)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9698, Prec@1: 76.01, Prec@5: 92.41
Epoch time: 82s
Saving models
Epoch: 199  lr: 0.001
Epoch: [199][000/391]	Time 1.0556 Data 0.8538 Loss 0.0173 (0.0173)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [199][100/391]	Time 0.1932 Data 0.0022 Loss 0.0152 (0.0157)	Acc@1 100.000 (99.977)	Acc@5 100.000 (100.000)
Epoch: [199][200/391]	Time 0.1899 Data 0.0023 Loss 0.0138 (0.0151)	Acc@1 100.000 (99.984)	Acc@5 100.000 (100.000)
Epoch: [199][300/391]	Time 0.1897 Data 0.0025 Loss 0.0097 (0.0154)	Acc@1 100.000 (99.974)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9750, Prec@1: 75.93, Prec@5: 92.36
Epoch time: 82s
Epoch: 200  lr: 0.001
Epoch: [200][000/391]	Time 1.0355 Data 0.7723 Loss 0.0103 (0.0103)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [200][100/391]	Time 0.1911 Data 0.0024 Loss 0.0154 (0.0152)	Acc@1 100.000 (99.961)	Acc@5 100.000 (100.000)
Epoch: [200][200/391]	Time 0.1890 Data 0.0021 Loss 0.0122 (0.0152)	Acc@1 100.000 (99.969)	Acc@5 100.000 (100.000)
Epoch: [200][300/391]	Time 0.1912 Data 0.0023 Loss 0.0119 (0.0153)	Acc@1 100.000 (99.971)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9700, Prec@1: 75.80, Prec@5: 92.46
Epoch time: 81s
Epoch: 201  lr: 0.001
Epoch: [201][000/391]	Time 1.0812 Data 0.8685 Loss 0.0134 (0.0134)	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)
Epoch: [201][100/391]	Time 0.1898 Data 0.0031 Loss 0.0142 (0.0150)	Acc@1 100.000 (99.977)	Acc@5 100.000 (100.000)
Epoch: [201][200/391]	Time 0.1892 Data 0.0021 Loss 0.0140 (0.0152)	Acc@1 100.000 (99.973)	Acc@5 100.000 (100.000)
Epoch: [201][300/391]	Time 0.1891 Data 0.0027 Loss 0.0169 (0.0155)	Acc@1 100.000 (99.974)	Acc@5 100.000 (100.000)
Testing the models......
Loss: 0.9738, Prec@1: 75.85, Prec@5: 92.38
